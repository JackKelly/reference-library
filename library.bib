% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{abbott2000synaptic-plasticity,
  Title                    = {Synaptic plasticity: taming the beast},
  Author                   = {Abbott, Larry F and Nelson, Sacha B},
  Year                     = {2000},
  Pages                    = {1178--1183},
  Volume                   = {3},

  File                     = {abbott2000synaptic-plasticity.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/abbott2000synaptic-plasticity.pdf:PDF},
  Journal                  = {Nature neuroscience},
  Keywords                 = {neuroscience, synaptic plasticity},
  Owner                    = {jack},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2015.03.21}
}

@Article{adams2007changepoint-detection,
  Title                    = {Bayesian online changepoint detection},
  Author                   = {Adams, Ryan Prescott and MacKay, David JC},
  Journaltitle             = {arXiv},
  Year                     = {2007},
  Eprint                   = {0710.3742},
  Eprinttype               = {arXiv},

  Abstract                 = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  Comment                  = {Python code is not available. Here's the MATLAB code: http://www.inference.phy.cam.ac.uk/rpa23/changepoint.php},
  Journal                  = {arXiv},
  Keywords                 = {machine learning, changepoint detection},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Article{akbari1995disaggregate-hourly,
  Title                    = {Validation of an algorithm to disaggregate whole-building hourly electrical load into end uses},
  Author                   = {Akbari, Hashem},
  Journaltitle             = {Energy},
  Year                     = {1995},
  Doi                      = {10.1016/0360-5442(95)00033-D},
  Number                   = {12},
  Pages                    = {1291--1301},
  Volume                   = {20},

  Abstract                 = {We have developed an algorithm to disaggregate short-interval (hourly) whole-building electrical load into major end uses. Hourly load data, hourly load-temperature regression coefficients and simulation end-use results comprise the algorithm input. The algorithm produces hourly load profiles for air conditioning, lighting, fans and pumps, and miscellaneous loads. Measured data from two end-use metered buildings (an office and a retail store) have been used to validate the algorithm. For the retail store, the algorithm estimates of hourly end use compare remarkably well with the monitored end-use data (average error of less than 5% during daytime operation). For the office building, the algorithm gives a consistent bias of about 12 and 27% in overestimating the HVAC and lighting electric loads, respectively, at the expense of underestimating the miscellaenous load by 35%. Results may be attributed to the presence of inconsistencies between office audit information and measured end-use data. A three-fold difference between the auditor's estimate for miscellaneous energy use and the metered amount has been found. The validation, however, indicates great promise for application of the algorithm to whole-building load data for obtaining reliable end-use data.},
  Keywords                 = {NILM, energy, very low freq},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.11.19}
}

@Inproceedings{amirach2014feature-extraction,
  Title                    = {A new approach for event detection and feature extraction for NILM},
  Author                   = {Amirach, Nabil and Xerri, Bernard and Borloz, Bruno and Jauffret, Claude},
  Booktitle                = {Electronics, Circuits and Systems (ICECS), 2014 21st IEEE International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {287--290},

  Keywords                 = {NILM, feature extraction, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Patent{amram2014historical,
  Title                    = {Historical utility consumption disaggregation},
  Author                   = {Amram, Martha and Sauder, Doug},
  Number                   = {US2011282506},
  Year                     = {2014},
  Month                    = {5},
  Url                      = {http://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=2011282506A1&KC=A1&FT=D&date=20111117&DB=EPODOC&locale=en_gb#},

  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Publisher                = {Google Patents},
  Timestamp                = {2015.11.19}
}

@Inproceedings{anderson2012blued,
  Title                    = {BLUED: A fully labeled public dataset for event-based non-intrusive load monitoring research},
  Author                   = {Anderson, Kyle and Ocneanu, Adrian and Benitez, Diego and Carlson, Derrick and Rowe, Anthony and Berges, Mario},
  Booktitle                = {Proceedings of the 2nd KDD workshop on data mining applications in sustainability (SustKDD)},
  Year                     = {2012},
  Pages                    = {1--5},

  Keywords                 = {NILM, energy, data},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{anderson2012event-detection,
  Title                    = {Event detection for non intrusive load monitoring},
  Author                   = {Anderson, Kyle D and Berg{\'e}s, Mario E and Ocneanu, Adrian and Benitez, Diego and Moura, Jos{\'e} MP},
  Booktitle                = {IECON 2012-38th Annual Conference on IEEE Industrial Electronics Society},
  Year                     = {2012},
  Doi                      = {10.1109/IECON.2012.6389367},
  Organization             = {IEEE},
  Pages                    = {3312--3317},

  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{arjunan2012sensoract,
  Title                    = {SensorAct: A Privacy and Security Aware Federated Middleware for Building Management},
  Author                   = {Arjunan, Pandarasamy and Batra, Nipun and Choi, Haksoo and Singh, Amarjeet and Singh, Pushpendra and Srivastava, Mani B.},
  Booktitle                = {Proceedings of the Fourth ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Buildings (BuildSys)},
  Year                     = {2012},
  Doi                      = {10.1145/2422531.2422547},
  ISBN                     = {978-1-4503-1170-0},
  Location                 = {Toronto, Ontario, Canada},
  Pages                    = {80--87},
  Publisher                = {ACM},
  Series                   = {BuildSys '12},

  Acmid                    = {2422547},
  Address                  = {New York, NY, USA},
  Keywords                 = {actuators, architecture, building management, deployment, middleware, sensors, energy},
  Numpages                 = {8},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Article{armel2013holy-grail,
  Title                    = {Is disaggregation the holy grail of energy efficiency? The case of electricity},
  Author                   = {Armel, K Carrie and Gupta, Abhay and Shrimali, Gireesh and Albert, Adrian},
  Journaltitle             = {Energy Policy},
  Year                     = {2013},
  Doi                      = {10.1016/j.enpol.2012.08.062},
  Pages                    = {213--234},
  Volume                   = {52},

  Abstract                 = {This paper aims to address two timely energy problems. First, significant low-cost energy reductions can be made in the residential and commercial sectors, but these savings have not been achievable to date. Second, billions of dollars are being spent to install smart meters, yet the energy saving and financial benefits of this infrastructure – without careful consideration of the human element – will not reach its full potential. We believe that we can address these problems by strategically marrying them, using disaggregation. Disaggregation refers to a set of statistical approaches for extracting end-use and/or appliance level data from an aggregate, or whole-building, energy signal. In this paper, we explain how appliance level data affords numerous benefits, and why using the algorithms in conjunction with smart meters is the most cost-effective and scalable solution for getting this data. We review disaggregation algorithms and their requirements, and evaluate the extent to which smart meters can meet those requirements. Research, technology, and policy recommendations are also outlined.},
  File                     = {armel2013holy-grail.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/armel2013holy-grail.pdf:PDF},
  Journal                  = {Energy Policy},
  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.09.07}
}

@Inproceedings{atlas1988artificial,
  Title                    = {An artificial neural network for spatio-temporal bipolar patterns: Application to phoneme classification},
  Author                   = {Atlas, Les E and Homma, Toshiteru and Marks II, Robert J},
  Booktitle                = {Proc. Neural Information Processing Systems (NIPS)},
  Year                     = {1988},
  Pages                    = {31},

  File                     = {atlas1988artificial.pdf:atlas1988artificial.pdf:PDF},
  Keywords                 = {neural networks, convolutional},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Article{auret2010changepoint-detection,
  Title                    = {Change point detection in time series data with random forests},
  Author                   = {Auret, Lidia and Aldrich, Chris},
  Year                     = {2010},
  Doi                      = {10.1016/j.conengprac.2010.04.005},
  Number                   = {8},
  Pages                    = {990--1002},
  Volume                   = {18},

  Abstract                 = {A large class of monitoring problems can be cast as the detection of a change in the parameters of a static or dynamic system, based on the effects of these changes on one or more observed variables. In this paper, the use of random forest models to detect change points in dynamic systems is considered. The approach is based on the embedding of multivariate time series data associated with normal process conditions, followed by the extraction of features from the resulting lagged trajectory matrix. The features are extracted by recasting the data into a binary classification problem, which can be solved with a random forest model. A proximity matrix can be calculated from the model and from this matrix features can be extracted that represent the trajectory of the system in phase space. The results of the study suggest that the random forest approach may afford distinct advantages over a previously proposed linear equivalent, particularly when complex nonlinear systems need to be monitored.},
  Journal                  = {Control Engineering Practice},
  Keywords                 = {changepoint detection, machine learning},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.11.19}
}

@Patent{ayachitula2010influencing,
  Title                    = {Influencing Consumer Behavior Modification with Utility Consumption Disaggregation},
  Author                   = {Ayachitula, Naga A and Chao, Tian-Jy and Dai, Jing D and Naphade, Milind R and Sahu, Sambit},
  Number                   = {US20100880915 20100913},
  Year                     = {2010},
  Url                      = {http://worldwide.espacenet.com/publicationDetails/biblio?FT=D&date=20110317&DB=EPODOC&locale=en_gb&CC=US&NR=2011066442A1&KC=A1},

  Abstract                 = {A method for performing utility consumption disaggregation includes measuring a total utility consumption of a consumer during a specified time period, generating a first disaggregated utility consumption segment and a second disaggregated utility consumption segment, based on the total utility consumption of the consumer, and providing the consumer with disaggregated utility consumption statistics based on at least one of the first and second disaggregated utility consumption segments.},
  Keywords                 = {NILM},
  Owner                    = {Jack},
  Publisher                = {Google Patents},
  Timestamp                = {2015.11.19}
}

@Article{balan2011parameter,
  Title                    = {Parameter identification and model based predictive control of temperature inside a house},
  Author                   = {B{\u{a}}lan, Radu and Cooper, Joshua and Chao, Kuo-Ming and Stan, Sergiu and Donca, Radu},
  Journaltitle             = {Energy and Buildings},
  Year                     = {2011},
  Doi                      = {10.1016/j.enbuild.2010.10.023},
  Number                   = {2},
  Pages                    = {748--758},
  Volume                   = {43},

  Abstract                 = {HVAC (Heating, Ventilation and Air Conditioning) systems used for heating or cooling buildings, consume a considerable amount of energy. To optimize the energy consumption, the behavior of occupants must be changed. This can be achieved by providing information and suggestions to occupants. A first step is developing of a less expensive and non-invasive measurement system and metering of the electricity and heat consumed. Based on collected experimental data, it can identify the parameters of a thermal model of the house. The model obtained will be used to simulate different aspects that can help to reduce the energy consumption. This paper presents a simple solution for thermal modeling of a house which includes experimental identification of the model's parameters. Such data are used to simulate the thermal behavior of the house and to obtain solutions to reduce energy consumption. In simulation, the control of the thermal system is performed using a model based predictive control algorithm.},
  Keywords                 = {energy, heat},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.11.19}
}

@Article{babaei2015dataset-directory,
  Title                    = {A study and a directory of energy consumption data sets of buildings},
  Author                   = {Babaei, Toktam and Abdi, Hamid and Lim, Chee Peng and Nahavandi, Saeid},
  Year                     = {2015},
  Doi                      = {10.1016/j.enbuild.2015.02.043},
  Pages                    = {91--99},
  Volume                   = {94},

  Abstract                 = {Energy consumption data are required to perform analysis, modelling, evaluation, and optimisation of energy usage in buildings. While a variety of energy consumption data sets have been examined and reported in the literature, there is a lack of a comprehensive categorisation and analysis of the available data sets. In this study, an overview of energy consumption data of buildings is provided. Three common strategies for generating energy consumption data, i.e., measurement, survey, and simulation, are described. A number of important characteristics pertaining to each strategy and the resulting data sets are discussed. In addition, a directory of energy consumption data sets of buildings is developed. The data sets are collected from either published papers or energy related organisations. The main contributions of this study include establishing a resource pertaining to energy consumption data sets and providing information related to the characteristics and availability of the respective data sets; therefore facilitating and promoting research activities in energy consumption data analysis.},
  Journal                  = {Energy and Buildings},
  Keywords                 = {energy, datasets, NILM},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.04.17}
}

@Inproceedings{baranski2004dynamic-programming,
  Title                    = {Detecting patterns of appliances from total load data using a dynamic programming approach},
  Author                   = {Baranski, Michael and Voss, J{\"u}rgen},
  Booktitle                = {Fourth IEEE International Conference on Data Mining (ICDM'04)},
  Year                     = {2004},
  Doi                      = {10.1109/ICDM.2004.10003},
  Organization             = {IEEE},
  Pages                    = {327--330},

  Abstract                 = {Nonintrusive appliance load monitoring (NIALM) systems require sufficient accurate total load data to separate the load into its major appliances. The most available solutions separate the whole electric energy consumption based on the measurement of all three voltages and currents. Aside from the cost for special measuring devices, the intrusion into the local installation is the main problem for reaching a high market distribution. The use of standard digital electricity meters could avoid this problem but the loss of information of the measured data has to be compensated by more intelligent algorithms and implemented rules to disaggregate the total load trace of only the active power measurements. The paper presents a NIALM approach to analyse data, collected from a standard digital electricity meter. To disaggregate the consumption of the entire active power into its major electrical end uses, an algorithm consisting of clustering methods, a genetic algorithm and a dynamic programming approach is presented. The genetic algorithm is used to combine frequently occurring events to create hypothetical finite state machines to model detectable appliances. The time series of each finite state machine is optimized using a dynamic programming method similar to the viterbi algorithm.},
  Keywords                 = {energy, NILM, dynamic programming},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{baranski2004genetic-algorithm,
  Title                    = {Genetic algorithm for pattern detection in NIALM systems},
  Author                   = {Baranski, Michael and Voss, J{\"u}rgen},
  Booktitle                = {Systems, Man and Cybernetics, 2004 IEEE International Conference on},
  Year                     = {2004},
  Doi                      = {10.1109/ICSMC.2004.1400878},
  Organization             = {IEEE},
  Pages                    = {3462--3468},
  Volume                   = {4},

  Keywords                 = {energy, NILM, genetic algorithm},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Book{barber2012bayesian,
  Title                    = {Bayesian reasoning and machine learning},
  Author                   = {Barber, David},
  Year                     = {2012},
  ISBN                     = {9780521518147},
  Publisher                = {Cambridge University Press},
  Url                      = {http://www.cs.ucl.ac.uk/staff/d.barber/brml/},

  Abstract                 = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  Keywords                 = {machine learning, bayesian},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Book{barber2011bayesian,
  Title                    = {Bayesian time series models},
  Author                   = {Barber, David and Cemgil, A Taylan and Chiappa, Silvia},
  Year                     = {2011},
  ISBN                     = {0521196760},
  Publisher                = {Cambridge University Press},

  Keywords                 = {machine learning, bayesian},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{barker2013electrical-loads,
  Title                    = {Empirical characterization and modeling of electrical loads in smart homes},
  Author                   = {Barker, Scott and Kalra, Sandeep and Irwin, David and Shenoy, Prashant},
  Booktitle                = {International Green Computing Conference (IGCC)},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {1--10},

  Abstract                 = {Smart meter deployments are spurring renewed interest in analysis techniques for electricity usage data. An important prerequisite for data analysis is characterizing and modeling how electrical loads use power. While prior work has made significant progress in deriving insights from electricity data, one issue that limits accuracy is the use of general and often simplistic load models. Prior models often associate a fixed power level with an “on” state and either no power, or some minimal amount, with an “off” state. This paper’s goal is to develop a new methodology for modeling electric loads that is both simple and accurate. Our approach is empirical in nature: we monitor a wide variety of common loads to distill a small number of common usage characteristics, which we leverage to construct accurate load-specific models. We show that our models are significantly more accurate than binary on-off models, decreasing the root mean square error by as much as 8X for representative loads. Finally, we demonstrate two example uses of our models in data analysis: i) generating device-accurate synthetic traces of building electricity usage, and ii) filtering out loads that generate rapid and random power variations in building electricity data},
  Keywords                 = {energy, electrical, simulation, appliances},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{barker2012smart,
  Title                    = {Smart*: An open data set and tools for enabling research in sustainable homes},
  Author                   = {Barker, Sean and Mishra, Aditya and Irwin, David and Cecchet, Emmanuel and Shenoy, Prashant and Albrecht, Jeannie},
  Booktitle                = {The 1st KDD Workshop on Data Mining Applications in Sustainability (SustKDD)},
  Year                     = {2012},

  Keywords                 = {NILM, data},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Article{barry1993bayesian,
  Title                    = {A Bayesian analysis for change point problems},
  Author                   = {Barry, Daniel and Hartigan, John A},
  Journaltitle             = {Journal of the American Statistical Association},
  Year                     = {1993},
  Number                   = {421},
  Pages                    = {309--319},
  Url                      = {http://www.jstor.org/stable/2290726},
  Volume                   = {88},

  Keywords                 = {machine learning, changepoint detection},
  Owner                    = {Jack},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2015.11.19}
}

@Misc{bastien2012theano,
  Title                    = {Theano: new features and speed improvements},
  Author                   = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
  Year                     = {2012},
  Eprint                   = {1211.5590},
  Eprinttype               = {arXiv},
  HowPublished             = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},

  Abstract                 = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.},
  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Inproceedings{batra2013occupancy,
  Title                    = {Experiences with Occupancy based Building Management Systems},
  Author                   = {Batra, Nipun and Arjunan, Pandarasamy and Singh, Ashutosh and Singh, Prashant},
  Booktitle                = {IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)},
  Year                     = {2013},
  Doi                      = {10.1109/ISSNIP.2013.6529781},
  Organization             = {IEEE},
  Pages                    = {153--158},

  Abstract                 = {Buildings are one of the largest consumers of electricity. Dominant electricity consumption within the buildings, contributed by plug loads, lighting and air conditioning, can be significantly improved using Occupancy-based Building Management Systems (Ob-BMS). In this paper, we address three critical aspects of Ob-BMS i.e. 1) Modular sensor node design to support diverse deployment scenarios; 2) Building architecture to support and scale fine resolution monitoring; and 3) Detailed analysis of the collected data for smarter actuation. We present key learning across these three aspects evolved over more than one year of design and deployment experiences. The sensor node design evolved over a period of time to address specific deployment requirements. With an opportunity at the host institute where two dorm buildings were getting constructed, we planned for the support infrastructure required for fine resolution monitoring embedded in the design phase and share our preliminary experiences and key learning thereof. Prototype deployment of the sensing system as per the planned support infrastructure was performed at two faculty offices with effective data collection worth 45 days. Collected data is analyzed accounting for efficient switching of appliances, in addition to energy conservation and user comfort as performed in the earlier occupancy based frameworks. Our analysis shows that occupancy prediction using simple heuristic based modeling can achieve similar performance as more complex Hidden Markov Models, thus simplifying the analytic framework.},
  Keywords                 = {energy, building management systems},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{batra2013indic,
  Title                    = {INDiC: Improved Non-Intrusive load monitoring using load Division and Calibration},
  Author                   = {Batra, Nipun and Dutta, Haimonti and Singh, Amarjeet},
  Booktitle                = {12th International Conference on Machine Learning and Applications (ICMLA)},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {79--84},
  Volume                   = {1},

  __markedentry            = {[Jack:]},
  Address                  = {Miami, FL, USA},
  Keywords                 = {energy, NILM},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{batra2014plc,
  Title                    = {Bits and Watts: Improving energy disaggregation performance using power line communication modems},
  Author                   = {Batra, Nipun and Gulati, Manoj and Jain, Puneet and Whitehouse, Kamin and Singh, Amarjeet},
  Booktitle                = {Proceedings of the First ACM International Conference on Embedded Systems For Energy-Efficient Buildings (BuildSys)},
  Year                     = {2014},
  Organization             = {ACM},

  Keywords                 = {energy, NILM, PLC},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{Batra:2013:DIH:2528282.2528293,
  Title                    = {It's Different: Insights into Home Energy Consumption in India},
  Author                   = {Batra, Nipun and Gulati, Manoj and Singh, Amarjeet and Srivastava, Mani B.},
  Booktitle                = {Proceedings of the 5th ACM Workshop on Embedded Systems For Energy-Efficient Buildings (BuildSys)},
  Year                     = {2013},
  Doi                      = {10.1145/2528282.2528293},
  ISBN                     = {978-1-4503-2431-1},
  Location                 = {Roma, Italy},
  Pages                    = {3:1--3:8},
  Publisher                = {ACM},
  Series                   = {BuildSys'13},

  Acmid                    = {2528293},
  Address                  = {New York, NY, USA},
  Articleno                = {3},
  Keywords                 = {energy, sensor networks, smart homes, deployment, buildings},
  Numpages                 = {8},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{NILMTK,
  Title                    = {NILMTK: An Open Source Toolkit for Non-intrusive Load Monitoring},
  Author                   = {Batra, Nipun and Kelly, Jack and Parson, Oliver and Dutta, Haimonti and Knottenbelt, William and Rogers, Alex and Singh, Amarjeet and Srivastava, Mani},
  Booktitle                = {Fifth International Conference on Future Energy Systems (ACM e-Energy)},
  Year                     = {2014},
  Doi                      = {10.1145/2602044.2602051},

  Address                  = {Cambridge, UK},
  File                     = {NILMTK.pdf:NILMTK.pdf:PDF},
  Keywords                 = {NILM, energy, NILMTK},
  Owner                    = {jack},
  Timestamp                = {2015.05.19}
}

@Article{batra2014comparison,
  Title                    = {A comparison of non-intrusive load monitoring methods for commercial and residential buildings},
  Author                   = {Batra, Nipun and Parson, Oliver and Berges, Mario and Singh, Amarjeet and Rogers, Alex},
  Journaltitle             = {arXiv preprint},
  Year                     = {2014},
  Eprint                   = {1408.6595},
  Eprinttype               = {arXiv},

  Journal                  = {arXiv preprint},
  Keywords                 = {energy, NILM},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Thesis{batra2014qualifier-report,
  Title                    = {Data Driven Energy Efficiency in Buildings},
  Author                   = {Batra, Nipun and Singh, Amarjeet and Singh, Pushpendra and Dutta, Haimonti and Sarangan, Venkatesh and Srivastava Srivastava, Mani},
  Institution              = {IIIT Delhi},
  Type                     = {PhD qualifier report},
  Year                     = {2014},
  Eprint                   = {1404.7227},
  Eprinttype               = {arXiv},

  Address                  = {IIIT Delhi},
  Booktitle                = {PhD qualifier report},
  Keywords                 = {energy},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Article{bengio2015biologically-plausible,
  Title                    = {Towards Biologically Plausible Deep Learning},
  Author                   = {Bengio, Yoshua and Dong-Hyun, Lee and Bornschein, Jorg and Lin, Zhouhan},
  Year                     = {2015},
  Eprint                   = {1502.04156},
  Eprinttype               = {arXiv},

  Abstract                 = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  File                     = {bengio2015biologically-plausible.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/bengio2015biologically-plausible.pdf:PDF},
  Keywords                 = {deep learning},
  Owner                    = {jack},
  Timestamp                = {2015.03.21}
}

@Unpublished{bengio2015book,
  Title                    = {Deep Learning},
  Author                   = {Yoshua Bengio and Ian J. Goodfellow and Aaron Courville},
  Year                     = {2015},
  Note                     = {Book in preparation for MIT Press},
  Url                      = {http://www.iro.umontreal.ca/~bengioy/dlbook},

  Keywords                 = {machine learning, deep learning},
  Owner                    = {jack},
  Timestamp                = {2015.03.30}
}

@Article{bengio2007scaling,
  Title                    = {Scaling learning algorithms towards {AI}},
  Author                   = {Bengio, Yoshua and LeCun, Yann and others},
  Journaltitle             = {Large-scale kernel machines},
  Year                     = {2007},
  Number                   = {5},
  Volume                   = {34},

  Journal                  = {Large-scale kernel machines},
  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Article{benitti2012robotics-in-schools,
  Title                    = {Exploring the educational potential of robotics in schools: A systematic review},
  Author                   = {Benitti, Fabiane Barreto Vavassori},
  Journaltitle             = {Computers & Education},
  Year                     = {2012},
  Doi                      = {10.1016/j.compedu.2011.10.006},
  Number                   = {3},
  Pages                    = {978--988},
  Volume                   = {58},

  Abstract                 = {This study reviews recently published scientific literature on the use of robotics in schools, in order to: (a) identify the potential contribution of the incorporation of robotics as educational tool in schools, (b) present a synthesis of the available empirical evidence on the educational effectiveness of robotics as an educational tool in schools, and (c) define future research perspectives concerning educational robotics. After systematically searching online bibliographic databases, ten relevant articles were located and included in the study. For each article, we analyze the purpose of the study, the content to be taught with the aid of robotics, the type of robot used, the research method used, and the sample characteristics (sample size, age range of students and/or level of education) and the results observed. The articles reviewed suggest that educational robotics usually acts as an element that enhances learning, however, this is not always the case, as there are studies that have reported situations in which there was no improvement in learning. The outcomes of the literature review are discussed in terms of their implications for future research, and can provide useful guidance for educators, practitioners and researchers in the area.},
  File                     = {benitti2012robotics-in-schools.pdf:benitti2012robotics-in-schools.pdf:PDF},
  Journal                  = {Computers \& Education},
  Keywords                 = {education, robotics},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.10.16}
}

@Inproceedings{benyoucef2010,
  Title                    = {Smart Meter with non-intrusive load monitoring for use in Smart Homes},
  Author                   = {Benyoucef, D. and Klein, P. and Bier, T.},
  Booktitle                = {IEEE International Energy Conference and Exhibition (EnergyCon)},
  Year                     = {2010},
  Doi                      = {10.1109/ENERGYCON.2010.5771810},
  ISBN                     = {978-1-4244-9378-4},
  Language                 = {English},
  Month                    = {Dec},
  Pages                    = {96--101},
  Publisher                = {IEEE},

  Abstract                 = {One feature of Smart Homes includes recording the consumption of electricity of appliances in the household. The recordings are conducted using Smart Meters. To demonstrate the usefulness of Smart Meters for the final customers, this paper identifies several benefits by presenting the research area non-intrusive appliance load monitoring (NALM). The purpose is to discuss the energy consumption of different appliances in detail in order to determine a means of reducing this consumption. The analysis focuses on the drawbacks of the appliances examined. Based on the results of the analysis, the approach to gathering data is described. With this approach we want to greatly simplify the NALM process. Finally the initial results obtained using this approach are discussed.},
  File                     = {benyoucef2010.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/benyoucef2010.pdf:PDF},
  Keywords                 = {energy, NILM},
  Owner                    = {jack},
  Timestamp                = {2015.11.20}
}

@Inproceedings{bergstra2010theano,
  Title                    = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  Author                   = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
  Booktitle                = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
  Year                     = {2010},
  Location                 = {Austin, TX},
  Month                    = {6},
  Note                     = {Oral Presentation},

  Abstract                 = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
  Keywords                 = {machine learning, software, open-source, deep learning},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Article{bers2014computational-thinking,
  Title                    = {Computational thinking and tinkering: Exploration of an early childhood robotics curriculum},
  Author                   = {Bers, Marina Umaschi and Flannery, Louise and Kazakoff, Elizabeth R and Sullivan, Amanda},
  Journaltitle             = {Computers & Education},
  Year                     = {2014},
  Doi                      = {10.1016/j.compedu.2013.10.020},
  Pages                    = {145--157},
  Volume                   = {72},

  Abstract                 = {By engaging in construction-based robotics activities, children as young as four can play to learn a range of concepts. The TangibleK Robotics Program paired developmentally appropriate computer programming and robotics tools with a constructionist curriculum designed to engage kindergarten children in learning computational thinking, robotics, programming, and problem-solving. This paper documents three kindergarten classrooms' exposure to computer programming concepts and explores learning outcomes. Results point to strengths of the curriculum and areas where further redesign of the curriculum and technologies would be appropriate. Overall, the study demonstrates that kindergartners were both interested in and able to learn many aspects of robotics, programming, and computational thinking with the TangibleK curriculum design.},
  File                     = {bers2014computational-thinking.pdf:bers2014computational-thinking.pdf:PDF},
  Journal                  = {Computers \& Education},
  Keywords                 = {education, robotics},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.10.16}
}

@Book{bishop2006PRML,
  Title                    = {Pattern recognition and machine learning},
  Author                   = {Christopher M. Bishop},
  Year                     = {2006},
  Publisher                = {springer New York},
  Volume                   = {1},

  Keywords                 = {machine learning},
  Owner                    = {Jack},
  Timestamp                = {2014.10.27}
}

@Techreport{Bishop94mixturedensity,
  Title                    = {Mixture density networks},
  Author                   = {Christopher M. Bishop},
  Institution              = {Aston University, Birmingham, UK},
  Year                     = {1994},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.120.5685},

  Keywords                 = {density estimation},
  Owner                    = {Jack},
  Timestamp                = {2014.10.27}
}

@Inproceedings{boulanger2012music-generation,
  Title                    = {Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription},
  Author                   = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  Booktitle                = {Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
  Year                     = {2012},
  Eprint                   = {1206.6392},
  Eprinttype               = {arXiv},

  File                     = {boulanger2012music-generation.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/boulanger2012music-generation.pdf:PDF},
  Keywords                 = {RNN-RBM, RNNs, generative, RBMs},
  Owner                    = {jack},
  Timestamp                = {2015.03.30}
}

@Article{buchanan2015energy-reduction,
  Title                    = {The question of energy reduction: The problem (s) with feedback},
  Author                   = {Buchanan, Kathryn and Russo, Riccardo and Anderson, Ben},
  Year                     = {2015},
  Doi                      = {10.1016/j.enpol.2014.12.008},
  Pages                    = {89--96},
  Volume                   = {77},

  Journal                  = {Energy Policy},
  Keywords                 = {energy, psychology, smart meters, user engagement},
  Owner                    = {jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.03.26}
}

@Article{bull2015moving-beyond-feedback,
  Title                    = {Moving beyond feedback: Energy behaviour and local engagement in the United Kingdom},
  Author                   = {Bull, Richard and Lemon, Mark and Everitt, Dave and Stuart, Graeme},
  Year                     = {2015},
  Doi                      = {10.1016/j.erss.2015.04.006},
  Pages                    = {32--40},
  Volume                   = {8},

  Abstract                 = {The energy savings potential within non-domestic buildings from behaviour change initiatives is well known. Energy efficiency measures can contribute to local, national and EU policy commitments on carbon reduction. Yet, research also shows behaviour change is anything but simple. No-where is this more evident that in local government where municipalities are expected to lead on carbon reduction initiatives whilst operating in challenging political landscapes. This paper reflects on a UK Research Council funded case study exploring the role of engagement in a UK municipality. Innovative feedback tools and user-engagement were developed in an effort to foster a collaborative approach to energy management.

Findings from an analysis of a focus group and a set of semi-structured interviews show encouraging signs with regard to increased user-engagement and digital tools, but barriers remain with regards to the ‘real world’ implementation of innovative, and technologically grounded, approaches. These included a staff reduction programme amidst financial cuts, a risk-averse culture with regard to new technologies, and debate about where responsibilities lie with regards to energy management. While these findings were case specific they have implications for organisations contemplating how technology might support them in workplace engagement for reduced energy use.},
  Journal                  = {Energy Research \& Social Science},
  Keywords                 = {energy, feedback, behaviour},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.06.11}
}

@Article{chandola2009anomaly-detection-survey,
  Title                    = {Anomaly Detection: A Survey},
  Author                   = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  Year                     = {2009},
  Doi                      = {10.1145/1541880.1541882},
  ISSN                     = {0360-0300},
  Month                    = {Jul},
  Number                   = {3},
  Pages                    = {15:1--15:58},
  Volume                   = {41},

  Acmid                    = {1541882},
  Address                  = {New York, NY, USA},
  Articleno                = {15},
  Issue_date               = {July 2009},
  Journal                  = {ACM Comput. Surv.},
  Keywords                 = {Anomaly detection, outlier detection},
  Numpages                 = {58},
  Owner                    = {Jack},
  Publisher                = {ACM},
  Timestamp                = {2014.10.30}
}

@Inproceedings{chang2011feature-extraction,
  Title                    = {Feature extraction of non-intrusive load-monitoring system using genetic algorithm in smart meters},
  Author                   = {Chang, Hsueh-Hsien and Chien, Po-Ching and Lin, Lung-Shu and Chen, Nanming},
  Booktitle                = {e-Business Engineering (ICEBE), 2011 IEEE 8th International Conference on},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {299--304},

  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Article{chorowski2014end-to-end-speech,
  Title                    = {End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results},
  Author                   = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  Year                     = {2014},
  Eprint                   = {1412.1602},
  Eprinttype               = {arXiv},

  File                     = {chorowski2014end-to-end-speech.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/chorowski2014end-to-end-speech.pdf:PDF},
  Keywords                 = {machine learning, deep learning, speech, attention},
  Owner                    = {jack},
  Timestamp                = {2015.03.30}
}

@Article{chung2015gated,
  Title                    = {Gated Feedback Recurrent Neural Networks},
  Author                   = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  Year                     = {2015},
  Eprint                   = {1502.02367},
  Eprinttype               = {arXiv},

  Abstract                 = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
  File                     = {chung2015gated.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/chung2015gated.pdf:PDF},
  Keywords                 = {RNNs},
  Owner                    = {jack},
  Timestamp                = {2015.03.21}
}

@Article{cichon2015dendritic-spikes,
  Title                    = {Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity},
  Author                   = {Cichon, Joseph and Gan, Wen-Biao},
  Date                     = {2015/03/30},
  Journaltitle             = {Nature},
  Year                     = {2015},
  Doi                      = {10.1038/nature14251},

  Abstract                 = {The brain has an extraordinary capacity for memory storage, but how it stores new information without disrupting previously acquired memories remains unknown. Here we show that different motor learning tasks induce dendritic Ca2+ spikes on different apical tuft branches of individual layer V pyramidal neurons in the mouse motor cortex. These task-related, branch-specific Ca2+ spikes cause long-lasting potentiation of postsynaptic dendritic spines active at the time of spike generation. When somatostatin-expressing interneurons are inactivated, different motor tasks frequently induce Ca2+ spikes on the same branches. On those branches, spines potentiated during one task are depotentiated when they are active seconds before Ca2+ spikes induced by another task. Concomitantly, increased neuronal activity and performance improvement after learning one task are disrupted when another task is learned. These findings indicate that dendritic-branch-specific generation of Ca2+ spikes is crucial for establishing long-lasting synaptic plasticity, thereby facilitating information storage associated with different learning experiences.},
  File                     = {cichon2015dendritic-spikes.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/cichon2015dendritic-spikes.pdf:PDF},
  Journal                  = {Nature},
  Keywords                 = {neuroscience, synaptic plasticity},
  Owner                    = {jack},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2015.04.08}
}

@Inproceedings{fabius2015variational,
  Title                    = {Variational Recurrent Auto-Encoders},
  Author                   = {Fabius, Otto and van Amersfoort, Joost R and Kingma, Diederik P},
  Booktitle                = {ICLR workshop track},
  Year                     = {2015},
  Eprint                   = {1412.6581},
  Eprinttype               = {arXiv},

  Abstract                 = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
  File                     = {fabius2015variational-recurrent-AE.pdf:fabius2015variational-recurrent-AE.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1412.6581},
  Keywords                 = {variational, autoencoder, recurrent},
  Owner                    = {Jack},
  Timestamp                = {2015.04.21}
}

@Article{felder2010wind,
  Title                    = {Wind power prediction using mixture density recurrent neural networks},
  Author                   = {Felder, Martin and Kaifel, Anton and Graves, Alex},
  Year                     = {2010},
  Url                      = {http://proceedings.ewea.org/ewec2010/allfiles2/413_EWEC2010presentation.pdf},

  File                     = {felder2010wind.pdf:felder2010wind.pdf:PDF},
  Journal                  = {Poster Presentation at the European Wind Energy Conference},
  Keywords                 = {mixture density networks, density estimation, RNNs},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Article{fukushima1980neocognitron,
  Title                    = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  Author                   = {Fukushima, Kunihiko},
  Journaltitle             = {Biological cybernetics},
  Year                     = {1980},
  Number                   = {4},
  Pages                    = {193--202},
  Volume                   = {36},

  File                     = {fukushima1980neocognitron.pdf:fukushima1980neocognitron.pdf:PDF},
  Journal                  = {Biological cybernetics},
  Keywords                 = {neural networks, convolutional},
  Owner                    = {Jack},
  Publisher                = {Springer},
  Timestamp                = {2015.07.23}
}

@Article{germain2015MADE,
  Title                    = {MADE: Masked Autoencoder for Distribution Estimation},
  Author                   = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  Year                     = {2015},

  File                     = {germain2015MADE.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/germain2015MADE.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1502.03509},
  Keywords                 = {density estimation},
  Owner                    = {jack},
  Timestamp                = {2015.03.20}
}

@Article{Gomez-Marin2014big-behavioural-data,
  Title                    = {Big behavioral data: psychology, ethology and the foundations of neuroscience},
  Author                   = {Gomez-Marin, Alex and Paton, Joseph J and Kampff, Adam R and Costa, Rui M and Mainen, Zachary F},
  Year                     = {2014},
  Doi                      = {10.1038/nn.3812},
  ISSN                     = {1097-6256},
  Language                 = {en},
  Month                    = {Oct},
  Number                   = {11},
  Pages                    = {1455--1462},
  Url                      = {http://www.nature.com/neuro/journal/v17/n11/full/nn.3812.html?WT.ec\_id=NEURO-201411},
  Volume                   = {17},

  Journal                  = {Nature Neuroscience},
  Keywords                 = {neuroscience, automatic-neuroscientist},
  Publisher                = {Nature Publishing Group}
}

@Inproceedings{goroshin2014unsupervised-feature-learning-from-temporal-data,
  Title                    = {Unsupervised Feature Learning from Temporal Data},
  Author                   = {Goroshin, Ross and Bruna, Joan and Szlam, Arthur and Tompson, Jonathan and Eigen, David and LeCun, Yann},
  Booktitle                = {Deep Learning and Representation Learning Workshop, NIPS},
  Year                     = {2014},

  Comment                  = {Related: goroshin2015unsupervised-feature-learning-from-temporal-data},
  File                     = {goroshin2014unsupervised-feature-learning-from-temporal-data.pdf:goroshin2014unsupervised-feature-learning-from-temporal-data.pdf:PDF},
  Keywords                 = {unsupervised learning, CNNs, timeseries, video},
  Owner                    = {Jack},
  Timestamp                = {2015.04.15}
}

@Article{goroshin2015unsupervised-feature-learning-from-temporal-data,
  Title                    = {Unsupervised Feature Learning from Temporal Data},
  Author                   = {Goroshin, Ross and Bruna, Joan and Tompson, Jonathan and Eigen, David and LeCun, Yann},
  Year                     = {2015},
  Eprint                   = {1504.02518},
  Eprinttype               = {arXiv},

  Comment                  = {Related: goroshin2015unsupervised-feature-learning-from-temporal-data},
  File                     = {goroshin2015unsupervised-feature-learning-from-temporal-data.pdf:goroshin2015unsupervised-feature-learning-from-temporal-data.pdf:PDF},
  Keywords                 = {unsupervised learning, CNNs, timeseries, video, machine learning},
  Owner                    = {Jack},
  Timestamp                = {2015.04.15}
}

@Article{graves2013generating-sequences,
  Title                    = {Generating sequences with recurrent neural networks},
  Author                   = {Alex Graves},
  Date                     = {2013/8/4},
  Journaltitle             = {arXiv preprint},
  Year                     = {2013},
  Eprint                   = {1308.0850},
  Eprinttype               = {arXiv},

  File                     = {:graves2013generating-sequences.pdf:PDF},
  Keywords                 = {LSTM},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Inproceedings{graves2012arabic-handwriting-recognition,
  Title                    = {Offline arabic handwriting recognition with multidimensional recurrent neural networks},
  Author                   = {Alex Graves},
  Booktitle                = {Guide to OCR for Arabic Scripts},
  Year                     = {2012},
  Pages                    = {297--313},
  Publisher                = {Springer London},

  File                     = {:graves2012arabic-handwriting-recognition.pdf:PDF},
  Keywords                 = {LSTM, handwriting, language},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Book{graves2012book-supervised-sequence-labelling,
  Title                    = {Supervised sequence labelling with recurrent neural networks},
  Author                   = {Alex Graves},
  Year                     = {2012},
  Publisher                = {Springer},
  Url                      = {http://www.cs.toronto.edu/~graves/preprint.pdf},
  Volume                   = {385},

  File                     = {:graves2012book-supervised-sequence-labelling.pdf:PDF},
  Keywords                 = {LSTM, RNNs},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Inproceedings{graves2014end-to-end-speech-recognition,
  Title                    = {Towards end-to-end speech recognition with recurrent neural networks},
  Author                   = {Alex Graves and Navdeep Jaitly},
  Booktitle                = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  Year                     = {2014},
  Pages                    = {1764-1772},

  File                     = {:graves2014end-to-end-speech-recognition.pdf:PDF},
  Keywords                 = {LSTM, RNNs, speech},
  Notes                    = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2014c2_graves14.pdf},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Inproceedings{graves2013speech-recognition-DBLSTM,
  Title                    = {Hybrid speech recognition with deep bidirectional LSTM},
  Author                   = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  Booktitle                = {Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on},
  Year                     = {2013},
  Doi                      = {10.1109/ASRU.2013.6707742},
  Eventdate                = {8-12 Dec. 2013},
  Location                 = {Olomouc},
  Pages                    = {273--278},

  File                     = {:graves2013speech-recognition-DBLSTM.pdf:PDF},
  Keywords                 = {LSTM, speech, language},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Article{graves2008CTC-unconstrained-handwriting-recognition,
  Title                    = {A novel connectionist system for unconstrained handwriting recognition},
  Author                   = {Graves, Alex and Liwicki, Marcus and Fern{\'a}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\"u}rgen},
  Year                     = {2008},
  Doi                      = {10.1109/TPAMI.2008.137},
  Number                   = {5},
  Pages                    = {855--868},
  Volume                   = {31},

  Abstract                 = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
  File                     = {:graves2008CTC-unconstrained-handwriting-recognition.pdf:PDF},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Keywords                 = {LSTM, handwriting, language},
  Owner                    = {jack},
  Publisher                = {IEEE},
  Timestamp                = {2014.10.22}
}

@Article{graves2014NTM,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and Greg Wayne and Ivo Danihelka},
  Year                     = {2014},
  Eprint                   = {1410.5401},
  Eprinttype               = {arXiv},

  Abstract                 = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  Archiveprefix            = {arXiv},
  Keywords                 = {LSTM, DeepMind},
  Owner                    = {jack},
  Timestamp                = {2014.10.28}
}

@Article{gregor2015DRAW,
  Title                    = {DRAW: A Recurrent Neural Network For Image Generation},
  Author                   = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
  Year                     = {2015},
  Eprint                   = {1502.04623},
  Eprinttype               = {arXiv},

  File                     = {gregor2015DRAW.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/gregor2015DRAW.pdf:PDF},
  Keywords                 = {RNNs, generative, DeepMind, variational autoencoder},
  Owner                    = {jack},
  Timestamp                = {2015.04.01}
}

@Book{hart1995,
  Title                    = {Nonintrusive appliance load monitoring with finite-state appliance models},
  Author                   = {George William Hart},
  Year                     = {1995},
  Language                 = {en},
  Publisher                = {Electric Power Research Institute},
  Url                      = {http://books.google.co.uk/books?id=OeJfMQAACAAJ},

  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Article{hart1992,
  Title                    = {Nonintrusive appliance load monitoring},
  Author                   = {George William Hart},
  Journaltitle             = {Proceedings of the IEEE},
  Year                     = {1992},
  Doi                      = {10.1109/5.192069},
  ISSN                     = {00189219},
  Language                 = {English},
  Month                    = {dec},
  Number                   = {12},
  Pages                    = {1870--1891},
  Volume                   = {80},

  Abstract                 = {A nonintrusive appliance load monitor that determines the energy consumption of individual appliances turning on and off in an electric load, based on detailed analysis of the current and voltage of the total load, as measured at the interface to the power source is described. The theory and current practice of nonintrusive appliance load monitoring are discussed, including goals, applications, load models, appliance signatures, algorithms, prototypes field-test results, current research directions, and the advantages and disadvantages of this approach relative to intrusive monitoring},
  File                     = {hart1992.pdf:hart1992.pdf:PDF},
  Journal                  = {Proceedings of the IEEE},
  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Publisher                = {IEEE},
  Timestamp                = {2015.07.22}
}

@Article{hart1989energy-monitoring,
  Title                    = {Residential energy monitoring and computerized surveillance via utility power flows},
  Author                   = {George William Hart},
  Journaltitle             = {IEEE Technology and Society Magazine},
  Year                     = {1989},
  Doi                      = {10.1109/44.31557},
  ISSN                     = {0278-0097},
  Language                 = {English},
  Month                    = {jun},
  Number                   = {2},
  Pages                    = {12--16},
  Url                      = {http://ieeexplore.ieee.org/ielx5/44/1367/00031557.pdf?tp=\&arnumber=31557\&isnumber=1367},
  Volume                   = {8},

  Abstract                 = {The author notes that all novel technologies have the potential to affect society in a complex manner, with both beneficial and detrimental consequences. He considers an illustrative case study: a nonintrusive appliance load monitoring technique that can provide vital information to help avoid future energy crises, but can also be used for surveillance purposes. He notes that there appears to be a significant potential for the technology to be abused. The danger that the technology might eventually lead to an erosion of civil liberties and privacy rights leaves its developers in an ethical quandary. The author examines how this technology should be controlled},
  File                     = {hart1989energy-monitoring.pdf:hart1989energy-monitoring.pdf:PDF},
  Journal                  = {IEEE Technology and Society Magazine},
  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Techreport{hart1985,
  Title                    = {Prototype Nonintrusive Appliance Load Monitor},
  Author                   = {George William Hart},
  Institution              = {MIT Energy Laboratory and Electric Power Research Institute},
  Year                     = {1985},
  Month                    = {sep},

  File                     = {hart1985.pdf:hart1985.pdf:PDF},
  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Patent{hart1989,
  Title                    = {Non-intrusive appliance monitor},
  Author                   = {George W. Hart and Edward C. Kern and Fred C. Schweppe},
  Number                   = {US4858141 A},
  Year                     = {1989},
  Url                      = {http://www.google.com/patents?vid=4858141},

  Abstract                 = {A non-intrusive monitor of energy consumption of residential appliances is described in which sensors, coupled to the power circuits entering a residence, supply analog voltage and current signals which are converted to digital format and processed to detect changes in certain residential load parameters, i.e., admittance. Cluster analysis techniques are employed to group change measurements into certain categories, and logic is applied to identify individual appliances and the energy consumed by each.},
  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Incollection{hermans2013analysing-RNNs,
  Title                    = {Training and Analysing Deep Recurrent Neural Networks},
  Author                   = {Hermans, Michiel and Schrauwen, Benjamin},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Editor                   = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  Year                     = {2013},
  Pages                    = {190--198},
  Publisher                = {Curran Associates, Inc.},
  Url                      = {http://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks.pdf},

  File                     = {hermans2013analysing-RNNs.pdf:hermans2013analysing-RNNs.pdf:PDF},
  Keywords                 = {RNNs},
  Owner                    = {Jack},
  Timestamp                = {2015.03.25}
}

@Article{hinton2006fast,
  Title                    = {A fast learning algorithm for deep belief nets},
  Author                   = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  Journaltitle             = {Neural computation},
  Year                     = {2006},
  Number                   = {7},
  Pages                    = {1527--1554},
  Volume                   = {18},

  Journal                  = {Neural computation},
  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Publisher                = {MIT Press},
  Timestamp                = {2015.07.23}
}

@Article{hinton2006reducing,
  Title                    = {Reducing the dimensionality of data with neural networks},
  Author                   = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  Year                     = {2006},
  Doi                      = {10.1126/science.1127647},
  Number                   = {5786},
  Pages                    = {504--507},
  Volume                   = {313},

  Abstract                 = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  File                     = {hinton2006reducing-dimensionality.pdf:hinton2006reducing-dimensionality.pdf:PDF},
  Journal                  = {Science},
  Keywords                 = {machine learning, autoencoder, RBMs,},
  Owner                    = {Jack},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2015.04.09}
}

@Inproceedings{hinton2014distilling,
  Title                    = {Distilling the knowledge in a neural network},
  Author                   = {Hinton, Geoffrey E and Vinyals, Oriol and Dean, Jeff},
  Booktitle                = {NIPS 2014 Deep Learning Workshop},
  Year                     = {2014},
  Eprint                   = {1503.02531},
  Eprinttype               = {arXiv},

  Abstract                 = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  File                     = {hinton2014distilling.pdf:hinton2014distilling.pdf:PDF},
  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Timestamp                = {2015.05.01}
}

@Article{hochreiter1997LSTM,
  Title                    = {Long short-term memory},
  Author                   = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  Journaltitle             = {Neural Computation},
  Year                     = {1997},
  Doi                      = {10.1162/neco.1997.9.8.1735},
  Number                   = {8},
  Pages                    = {1735--1780},
  Volume                   = {9},

  File                     = {:hochreiter1997LSTM.pdf:PDF},
  Journal                  = {Neural Computation},
  Keywords                 = {LSTM},
  Owner                    = {jack},
  Publisher                = {MIT Press},
  Timestamp                = {2014.10.22}
}

@Article{ioffe2015batch-normalization,
  Title                    = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  Author                   = {Ioffe, Sergey and Szegedy, Christian},
  Year                     = {2015},
  Eprint                   = {1502.03167},
  Eprinttype               = {arXiv},

  Abstract                 = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  File                     = {ioffe2015batch-normalization.pdf:ioffe2015batch-normalization.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1502.03167},
  Keywords                 = {deep learning},
  Owner                    = {Jack},
  Timestamp                = {2015.04.23}
}

@Article{jaderberg2015STN,
  Title                    = {Spatial Transformer Networks},
  Author                   = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  Year                     = {2015},
  Eprint                   = {1506.02025},
  Eprinttype               = {arXiv},

  Abstract                 = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  Comment                  = {Related: sonderby2015RNN-SPN},
  File                     = {jaderberg2015STN.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/jaderberg2015STN.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1506.02025},
  Keywords                 = {deep learning, spatial transformer networks},
  Owner                    = {jack},
  Timestamp                = {2015.09.23}
}

@Phdthesis{jaitly2014thesis,
  Title                    = {Exploring Deep Learning Methods for discovering features in speech signals.},
  Author                   = {Jaitly, Navdeep},
  Institution              = {University of Toronto},
  Year                     = {2014},

  File                     = {jaitly2014thesis.pdf:jaitly2014thesis.pdf:PDF},
  Keywords                 = {deep learning, RBMs, speech, HMMs, capsules},
  Owner                    = {Jack},
  Timestamp                = {2015.03.30}
}

@Techreport{jeffery2015climate,
  Title                    = {How close are INDCs to 2 and 1.5°C pathways?},
  Author                   = {Louise Jeffery and Ryan Alexander and Bill Hare and Marcia Rocha and Michiel Schaeffer and Niklas Höhne and Hanna Fekete and Pieter van Breevoort and Kornelis Blok},
  Date                     = {1/09/2015},
  Institution              = {Climate Action Tracker},
  Year                     = {2015},
  Subtitle                 = {Climate Action Tracker Update},
  Url                      = {http://climateactiontracker.org/assets/publications/briefing_papers/CAT_EmissionsGap_Briefing_Sep2015.pdf},

  Abstract                 = {Aggregate INDC emissions are far above levels consistent with below
2°C, with around 65% of global emissions covered
INDCs announced by 1 September 2015 lead to global emissions far above the levels
needed by 2025 and 2030 to put the world on track to hold warming below 2°C, or to below
1.5°C, in 2100.
As of 1st September, 29 INDC submissions have been received, reflecting 56 countries
(including the European Union member states), and covering around 65% of global
emissions in 2010 (excluding LULUCF) and 43% of global population. The CAT has directly
assessed 16 of these INDCs covering 64.5% of global emissions in 2010 (excluding LULUCF)
and 41% of global population.
With the INDCs submitted to date, the CAT projects total global emissions are on track to
be 53-57 GtCO2e in 2025 and 55-59 GtCO2e in 2030, far above the least-cost global
pathways consistent with limiting warming below 2°C. Additional reductions in the order of
12-15 GtO2e by 2025 and of 17-21 GtCO2e by 2030 are needed for global emissions to be
consistent with a 2°C pathway.
INDCs are yet to come from 140 countries. The ten highest emitters yet to submit INDCs
are India, Brazil, Iran, Indonesia, Saudi Arabia, South Africa, Thailand, Turkey, Ukraine, and
Pakistan, together accounting for 18% of global emissions not yet covered by INDCs
(excluding LULUCF).
Aside from the insufficient ambition of the aggregate INDCs, there is a significant gap
between current policies and the INDCs: global emissions under currently implemented
policies are projected to be higher than the already inadequate INDC levels. Some countries
propose INDCs close to the current trajectory giving confidence that they are met (e.g. EU
and China). Others have put forward a target that would be a significant change in trend, but these are not yet supported by any significant existing legislation, e.g. Australia and
Canada, raising questions about the likely implementation. Yet others are showing progress
in policy implementation, continuously moving their future trajectories downwards, but
policies are not yet sufficient to meet their (still inadequate) INDCs (e.g. USA).
The gap between pledges and policies increases through time, highlighting the need for
long-term policy action. This is not to underplay the significance and importance of
governments putting in place policies that will actually reduce their emissions, but for many
governments this is not yet the case.},
  Keywords                 = {climate change, environment, carbon emissions},
  Owner                    = {jack},
  Timestamp                = {2015.09.07}
}

@Thesis{kelly2011msc-thesis,
  Title                    = {Disaggregating Smart Meter Readings using Device Signatures},
  Author                   = {Jack Kelly},
  Institution              = {Imperial College London},
  Type                     = {Computing MSc Thesis},
  Year                     = {2011},
  Month                    = {September},
  Url                      = {http://www.doc.ic.ac.uk/teaching/distinguished-projects/2011/d.kelly.pdf},

  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{kelly2014NILMTKv02,
  Title                    = {NILMTK v0.2: A Non-intrusive Load Monitoring Toolkit for Large Scale Data Sets},
  Author                   = {Kelly, Jack and Batra, Nipun and Parson, Oliver and Dutta, Haimonti and Knottenbelt, William and Rogers, Alex and Singh, Amarjeet and Srivastava, Mani},
  Booktitle                = {Proceedings of the First ACM International Conference on Embedded Systems For Energy-Efficient Buildings (BuildSys)},
  Year                     = {2014},
  Doi                      = {10.1145/2674061.2675024},
  Eprint                   = {1409.5908},
  Eprinttype               = {arXiv},
  Location                 = {Memphis, TN, USA},
  Organization             = {ACM},

  Address                  = {Memphis, USA},
  File                     = {kelly2014NILMTKv02.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/kelly2014NILMTKv02.pdf:PDF},
  Keywords                 = {NILM, NILMTK, energy},
  Owner                    = {Jack},
  Timestamp                = {2014.11.07}
}

@Inproceedings{NeuralNILM,
  Title                    = {Neural NILM: Deep Neural Networks Applied to Energy Disaggregation},
  Author                   = {Jack Kelly and William Knottenbelt},
  Booktitle                = {Proceedings of the Second ACM Workshop On Embedded Systems For Energy-Efficient Buildings (BuildSys)},
  Year                     = {2015},
  Doi                      = {10.1145/2821650.2821672},
  Eprint                   = {1507.06594},
  Eprinttype               = {arXiv},
  Location                 = {Seoul, South Korea},
  Month                    = {November},
  Organization             = {ACM},

  Keywords                 = {energy, NILM, neural networks, deep learning},
  Owner                    = {Jack},
  Timestamp                = {2015.09.07}
}

@Article{UK-DALE,
  Title                    = {The {UK-DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five UK homes},
  Author                   = {Jack Kelly and William Knottenbelt},
  Date                     = {2015/03/31},
  Journaltitle             = {Scientific Data},
  Year                     = {2015},
  Doi                      = {10.1038/sdata.2015.7},
  Eprint                   = {1404.0284},
  Eprinttype               = {arXiv},
  Number                   = {150007},
  Volume                   = {2},

  File                     = {UK-DALE.pdf:UK-DALE.pdf:PDF},
  Journal                  = {Scientific Data},
  Keywords                 = {data, NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.03.09}
}

@Inproceedings{kelly2014metadata,
  Title                    = {Metadata for Energy Disaggregation},
  Author                   = {Kelly, Jack and Knottenbelt, William},
  Booktitle                = {Computer Software and Applications Conference Workshop (COMPSACW) at the 2nd IEEE International Workshop on Consumer Devices and Systems (CDS)},
  Year                     = {2014},
  Doi                      = {10.1109/COMPSACW.2014.97},
  Eprint                   = {1403.5946},

  Address                  = {V{\" a}ster{\aa}s, Sweden},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1403.5946},
  File                     = {kelly2014metadata.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/kelly2014metadata.pdf:PDF},
  Keywords                 = {NILM, metadata, energy},
  Owner                    = {jack},
  Timestamp                = {2015.05.19}
}

@Inproceedings{kingma2014semi-supervised,
  Title                    = {Semi-supervised Learning with Deep Generative Models},
  Author                   = {Kingma, Diederik P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Editor                   = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
  Year                     = {2014},
  Pages                    = {3581--3589},
  Publisher                = {Curran Associates, Inc.},
  Url                      = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf},

  File                     = {kingma2014semi-supervised.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/kingma2014semi-supervised.pdf:PDF},
  Keywords                 = {DeepMind, generative, semi-supervised},
  Owner                    = {jack},
  Timestamp                = {2015.03.30}
}

@Article{kingma2013VAE,
  Title                    = {Auto-encoding variational bayes},
  Author                   = {Kingma, Diederik P and Welling, Max},
  Year                     = {2013},

  Abstract                 = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  File                     = {kingma2013VAE.pdf:kingma2013VAE.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1312.6114},
  Keywords                 = {autoencoder, variational},
  Owner                    = {Jack},
  Timestamp                = {2015.05.21}
}

@Inproceedings{kiros2014unifying,
  Title                    = {Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models},
  Author                   = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S},
  Booktitle                = {NIPS 2014 deep learning workshop},
  Year                     = {2014},
  Eprint                   = {1411.2539},
  Eprinttype               = {arXiv},

  Keywords                 = {LSTM},
  Owner                    = {jack},
  Timestamp                = {2014.11.17}
}

@Inproceedings{kolter2011REDD,
  Title                    = {{REDD}: A public data set for energy disaggregation research},
  Author                   = {Kolter, J Zico and Johnson, Matthew J},
  Booktitle                = {Workshop on Data Mining Applications in Sustainability (SIGKDD), San Diego, CA},
  Year                     = {2011},
  Organization             = {Citeseer},
  Pages                    = {59--62},
  Volume                   = {25},

  Keywords                 = {energy, data, NILM},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Article{koutnik2014clockwork,
  Title                    = {A Clockwork RNN},
  Author                   = {Koutn{\'\i}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  Year                     = {2014},
  Eprint                   = {1402.3511},
  Eprinttype               = {arXiv},

  File                     = {koutnik2014clockwork.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/koutnik2014clockwork.pdf:PDF},
  Keywords                 = {RNNs},
  Owner                    = {jack},
  Timestamp                = {2015.03.31}
}

@Inproceedings{Koutnik2014evolving,
  Title                    = {Evolving deep unsupervised convolutional networks for vision-based reinforcement learning},
  Author                   = {Koutn{\'\i}k, Jan and Schmidhuber, J{\"u}rgen and Gomez, Faustino},
  Booktitle                = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
  Year                     = {2014},
  Doi                      = {10.1145/2576768.2598358},
  ISBN                     = {9781450326629},
  Month                    = {Jul},
  Pages                    = {541--548},
  Publisher                = {ACM Press},
  Url                      = {http://dl.acm.org/citation.cfm?id=2576768.2598358},

  Address                  = {New York, New York, USA},
  Keywords                 = {deep learning,games,neuroevolution,reinforcement learning,vision-based torcs},
  Owner                    = {Jack},
  Timestamp                = {2014.10.28}
}

@Inproceedings{krizhevsky2012imagenet,
  Title                    = {ImageNet Classification with Deep Convolutional Neural Networks},
  Author                   = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Editor                   = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
  Year                     = {2012},
  Pages                    = {1097--1105},
  Publisher                = {Curran Associates, Inc.},

  File                     = {krizhevsky2012imagenet.pdf:krizhevsky2012imagenet.pdf:PDF},
  Keywords                 = {machine learning, CNNs, image classification, deep learning},
  Owner                    = {Jack},
  Timestamp                = {2014.11.12}
}

@Article{larochelle2011NADE,
  Title                    = {The Neural Autoregressive Distribution Estimator},
  Author                   = {Hugo Larochelle and Iain Murray},
  Year                     = {2011},
  Pages                    = {29--37},
  Volume                   = {15},

  File                     = {larochelle2011NADE.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/larochelle2011NADE.pdf:PDF},
  Journal                  = {JMLR: W\&CP},
  Keywords                 = {density estimation},
  Owner                    = {jack},
  Timestamp                = {2015.03.20}
}

@Article{le2015ReLU-RNNs,
  Title                    = {A Simple Way to Initialize Recurrent Networks of Rectified Linear Units},
  Author                   = {Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  Year                     = {2015},
  Eprint                   = {1504.00941},
  Eprinttype               = {arXiv},

  Abstract                 = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
  File                     = {le2015ReLU-RNNs.pdf:le2015ReLU-RNNs.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1504.00941},
  Keywords                 = {RNNs, ReLU, LSTM, initialisation},
  Owner                    = {Jack},
  Timestamp                = {2015.04.23}
}

@Article{lecun1998gradient,
  Title                    = {Gradient-based learning applied to document recognition},
  Author                   = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  Year                     = {1998},
  Number                   = {11},
  Pages                    = {2278--2324},
  Volume                   = {86},

  Journal                  = {Proceedings of the IEEE},
  Keywords                 = {neural networks, convolutional},
  Owner                    = {Jack},
  Publisher                = {IEEE},
  Timestamp                = {2015.07.23}
}

@Inproceedings{lee2009unsupervised,
  Title                    = {Unsupervised feature learning for audio classification using convolutional deep belief networks},
  Author                   = {Lee, Honglak and Pham, Peter and Largman, Yan and Ng, Andrew Y},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2009},
  Pages                    = {1096--1104},

  Abstract                 = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. For the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
  File                     = {lee2009unsupervised.pdf:lee2009unsupervised.pdf:PDF},
  Keywords                 = {deep belief networks, audio, classification, unsupervised},
  Owner                    = {Jack},
  Timestamp                = {2015.04.17}
}

@Article{leeb1995transient,
  Title                    = {Transient event detection in spectral envelope estimates for nonintrusive load monitoring},
  Author                   = {Leeb, Steven B and Shaw, Steven R and Kirtley Jr, JJames L},
  Journaltitle             = {IEEE Transactions on Power Delivery},
  Year                     = {1995},
  Doi                      = {10.1109/61.400897},
  Number                   = {3},
  Pages                    = {1200--1210},
  Volume                   = {10},

  Abstract                 = {This paper describes the theoretical foundation and prototype implementation of a power system transient event detector for use in a nonintrusive load monitor (NILM). The NILM determines the operating schedule of the major electrical loads in a building from measurements made at the electric utility service entry. The transient event detector extends the applicability of the NILM to challenging commercial and industrial sites. A spectral preprocessor for use in the transient event detector is introduced first. Then, the transient event detection algorithm is developed. The performance of the algorithm is illustrated with results from a prototype event detector},
  File                     = {leeb1995transient.pdf:leeb1995transient.pdf:PDF},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Keywords                 = {NILM, feature extraction, event detection, energy},
  Owner                    = {Jack},
  Publisher                = {IEEE},
  Timestamp                = {2015.07.23}
}

@Inproceedings{lin2010feature-extraction,
  Title                    = {A novel feature extraction method for the development of nonintrusive load monitoring system based on {BP-ANN}},
  Author                   = {Lin, Yu-Hsiu and Tsai, Men-Shen},
  Booktitle                = {2010 International Symposium on Computer Communication Control and Automation (3CA)},
  Year                     = {2010},
  Doi                      = {10.1109/3CA.2010.5533571},
  Organization             = {IEEE},
  Pages                    = {215--218},
  Volume                   = {2},

  Abstract                 = {The novel feature extraction method for nonintrusive load monitoring (NILM) systems was proposed in this paper. In order to monitoring the status of each load, a sensor is installed for each load traditionally. The status of the load is transmitted to the main controller such that the status of each load can be monitored. On the other hand, the NILM is able to detect the status of loads by analyzing the current signals that is picked-up by a current sensor installed at the main electrical panel. Traditional NILM methods used real power, reactive power, harmonic contents of power signatures and transient energy to determine the status of appliances. These methods are very complex and require a lot of computation. In this paper, a novel method that integrates artificial intelligent recognition technique and load current acquisition method for NILM is proposed. The proposed method uses timedomain information. This approach is different from traditional NILM methods. The proposed method is able to detect the energization and de-energization of loads by applying back-propagation neural networks (BP-ANNs). The overall correct rate for this method is above 98.75%. This result shows that the proposed method is able to determine the operation status of loads with proper robustness.},
  File                     = {lin2010feature-extraction.pdf:lin2010feature-extraction.pdf:PDF},
  Keywords                 = {NILM, neural networks, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Inproceedings{lowe1999SIFT,
  Title                    = {Object recognition from local scale-invariant features},
  Author                   = {Lowe, David G},
  Booktitle                = {Computer vision, 1999. The proceedings of the seventh {IEEE} international conference on},
  Year                     = {1999},
  Organization             = {IEEE},
  Pages                    = {1150--1157},
  Volume                   = {2},

  File                     = {lowe1999SIFT.pdf:lowe1999SIFT.pdf:PDF},
  Keywords                 = {computer vision, feature detection},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Inproceedings{maas2012RNN-noise-reduction-in-ASR,
  Title                    = {Recurrent Neural Networks for Noise Reduction in Robust ASR},
  Author                   = {Maas, Andrew L and Le, Quoc V and O'Neil, Tyler M and Vinyals, Oriol and Nguyen, Patrick and Ng, Andrew Y},
  Booktitle                = {INTERSPEECH},
  Year                     = {2012},
  Organization             = {Citeseer},
  Url                      = {http://www1.icsi.berkeley.edu/~vinyals/Files/rnn_denoise_2012.pdf},

  Abstract                 = {Recent work on deep neural networks as acoustic mod-
els for automatic speech recognition (ASR) have demon-
strated substantial performance improvements. We intro-
duce a model which uses a deep recurrent auto encoder
neural network to denoise input features for robust ASR.
The model is trained on stereo (noisy and clean) audio
features to predict clean features given noisy input. The
model makes no assumptions about how noise affects the
signal, nor the existence of distinct noise environments.
Instead, the model can learn to model any type of distor-
tion or additive noise given sufficient training data. We
demonstrate the model is competitive with existing fea-
ture denoising approaches on the Aurora2 task, and out-
performs a tandem approach where deep networks are
used to predict phoneme posteriors directly},
  Comment                  = {Related: maas2013RNN-feature-enhancement},
  File                     = {maas2012RNN-noise-reduction-in-ASR.pdf:maas2012RNN-noise-reduction-in-ASR.pdf:PDF},
  Keywords                 = {RNN, recurrent, speech, ASR, autoencoder, DRDAE},
  Owner                    = {Jack},
  Timestamp                = {2015.04.21}
}

@Inproceedings{maas2013RNN-feature-enhancement,
  Title                    = {Recurrent neural network feature enhancement: The 2nd CHiME challenge},
  Author                   = {Maas, Andrew L and O'Neil, Tyler M and Hannun, Awni Y and Ng, Andrew Y},
  Booktitle                = {Proceedings The 2nd CHiME Workshop on Machine Listening in Multisource Environments held in conjunction with ICASSP},
  Year                     = {2013},
  Pages                    = {79--80},
  Url                      = {http://web.stanford.edu/~awni/papers/drdae_chime2013_final.pdf},

  Abstract                 = {We apply a machine learning approach to improve noisy
acoustic features for robust speech recognition. Specifically, we train a deep, recurrent neural network to map noise-
corrupted input features to their corresponding clean ver-
sions. We introduce several improvements to previously pro-
posed neural network feature enhancement architectures. The
model does not include assumptions about the specific noise
and distortions present in CHiME data, but does assume noisy
and clean stereo pairs are available for training. When used
with the standard recognizer on the small vocabulary task
(track 1), our approach demonstrates substantial improve-
ments over the challenge baseline.},
  Comment                  = {Related: maas2012RNN-noise-reduction-in-ASR},
  File                     = {maas2013RNN-feature-enhancement.pdf:maas2013RNN-feature-enhancement.pdf:PDF},
  Keywords                 = {RNN, recurrent, speech, ASR, autoencoder, DRDAE},
  Owner                    = {Jack},
  Timestamp                = {2015.04.21}
}

@Inproceedings{makriyiannis2014argumentation,
  Title                    = {Smarter Electricity through Argumentation},
  Author                   = {Makriyiannis, Menelaos and Lung, Tudor and Craven, Robert and Toni, Francesca and Kelly, Jack},
  Booktitle                = {4th International Workshop on Combinations of Intelligent Methods and Applications in conjunction with the IEEE International Conference on Tools with AI},
  Year                     = {2014},

  File                     = {makriyiannis2014argumentation.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/makriyiannis2014argumentation.pdf:PDF},
  Keywords                 = {energy, recommender systems},
  Owner                    = {jack},
  Timestamp                = {2015.05.19}
}

@Incollection{masci2011stacked,
  Title                    = {Stacked convolutional auto-encoders for hierarchical feature extraction},
  Author                   = {Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
  Booktitle                = {Artificial Neural Networks and Machine Learning--ICANN 2011},
  Year                     = {2011},
  Pages                    = {52--59},
  Publisher                = {Springer},

  Abstract                 = {We present a novel convolutional auto-encoder (CAE) for
unsupervised feature learning. A stack of CAEs forms a convolutional
neural network (CNN). Each CAE is trained using conventional on-line
gradient descent without additional regularization terms. A max-pooling
layer is essential to learn biologically plausible features consistent with
those found by previous approaches. Initializing a CNN with filters of a
trained CAE stack yields superior performance on a digit (MNIST) and
an object recognition (CIFAR10) benchmark.},
  File                     = {masci2011stacked-convolutional-auto-encoders.pdf:masci2011stacked-convolutional-auto-encoders.pdf:PDF},
  Keywords                 = {autoencoder, convolutional},
  Owner                    = {Jack},
  Timestamp                = {2015.05.01}
}

@Techreport{mayhorn2015characteristics,
  Title                    = {Characteristics and Performance of Existing Load Disaggregation Technologies},
  Author                   = {Mayhorn, E. T. and Butner, R.S. and Baechler, M. C. and Sullivan, G.P. and Hao, H.},
  Institution              = {Pacific Northwest National Laboratory (PNNL), Richland, WA (US)},
  Year                     = {2015},
  Url                      = {http://www.pnnl.gov/main/publications/external/technical_reports/PNNL-24230.pdf},

  File                     = {mayhorn2015characteristics.pdf:mayhorn2015characteristics.pdf:PDF},
  Keywords                 = {NILM, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.11.09}
}

@Inproceedings{michalski2014grammar-cells,
  Title                    = {Modeling Deep Temporal Dependencies with Recurrent "Grammar Cells"},
  Author                   = {Michalski, Vincent and Memisevic, Roland and Konda, Kishore},
  Booktitle                = {Advances in Neural Information Processing Systems (NIPS 2014)},
  Year                     = {2014},
  Eprint                   = {1402.2333},
  Eprinttype               = {arXiv},
  Pages                    = {1925--1933},
  Url                      = {http://www.iro.umontreal.ca/~memisevr/pubs/predictive2014.pdf},

  File                     = {michalski2014grammar-cells.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/michalski2014grammar-cells.pdf:PDF},
  Keywords                 = {RNNs},
  Owner                    = {jack},
  Timestamp                = {2015.04.01}
}

@Article{mnih2013playing-atari,
  Title                    = {Playing Atari with deep reinforcement learning},
  Author                   = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  Journaltitle             = {arXiv preprint},
  Year                     = {2013},
  Eprint                   = {1312.5602},
  Eprinttype               = {arXiv},

  File                     = {:mnih2013playing-atari.pdf:PDF},
  Keywords                 = {DeepMind, deep learning},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Article{mnih2015deep-reinforcement-learning,
  Title                    = {Human-level control through deep reinforcement learning},
  Author                   = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  Journaltitle             = {Nature},
  Year                     = {2015},
  Number                   = {7540},
  Pages                    = {529--533},
  Volume                   = {518},

  Journal                  = {Nature},
  Keywords                 = {machine learning, deep learning, deep reinforcement learning},
  Owner                    = {Jack},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2015.07.23}
}

@Www{nouri2014facial-keypoints,
  Title                    = {Using convolutional neural nets to detect facial keypoints tutorial},
  Author                   = {Daniel Nouri},
  Url                      = {http://bit.ly/1OduG83},
  Year                     = {2014},

  Keywords                 = {deep learning, vision, regression},
  Owner                    = {jack},
  Timestamp                = {2015.07.18}
}

@Article{oleary2015thermal-efficiency,
  Title                    = {Review and evaluation of using household metered energy data for rating of building thermal efficiency of existing buildings},
  Author                   = {Timothy O’Leary and M. Belusko and D. Whaley and F. Bruno},
  Year                     = {2015},
  Doi                      = {10.1016/j.enbuild.2015.09.018},
  ISSN                     = {0378-7788},
  Pages                    = {433 - 440},
  Volume                   = {108},

  Abstract                 = {Abstract This paper investigates the use of actual monitored household energy as an indicator of the thermal efficiency of a dwelling and subsequently rating of the building thermal performance. The paper reviews evaluation methods used internationally for both building thermal efficiency and building energy labelling and presents results from two discrete studies in South Australia on monitoring actual household energy consumption. In order to investigate the occupancy effect on household energy, monitored energy data collected from two different housing developments in South Australia were examined. The energy ratings for these homes are compliant with the national agreed protocols for thermal performance modelling of dwellings, where one set of homes is a group occupied by higher socio-economic groups and the other is low income public housing in a colder climate region with much poorer home energy ratings. The wide variation of actual household energy for the homes that have relatively similar thermal envelopes indicates a lack of meaningful use for actual household energy in disclosure of house energy performance. Therefore, it is argued that thermal modelling software used to rate homes appears a more useful application of a system of disclosure of energy performance than the use of energy bills.},
  File                     = {oleary2015thermal-efficiency.pdf:oleary2015thermal-efficiency.pdf:PDF},
  Journal                  = {Energy and Buildings},
  Keywords                 = {Monitored energy use, energy, thermal, efficiency, comfort, behaviour}
}

@Article{olazaran1996perceptrons-controversy,
  Title                    = {A sociological study of the official history of the perceptrons controversy},
  Author                   = {Olazaran, Mikel},
  Year                     = {1996},
  Number                   = {3},
  Pages                    = {611--659},
  Url                      = {http://www.jstor.org/stable/285702},
  Volume                   = {26},

  File                     = {olazaran1996perceptrons-controversy.pdf:olazaran1996perceptrons-controversy.pdf:PDF},
  Journal                  = {Social Studies of Science},
  Keywords                 = {history, perceptrons},
  Owner                    = {Jack},
  Publisher                = {Sage Publications},
  Timestamp                = {2015.04.01}
}

@Inproceedings{parson2015dataport,
  Title                    = {Dataport and {NILMTK}: A building data set designed for non-intrusive load monitoring},
  Author                   = {Parson, Oliver and Fisher, Grant and Hersey, April and Batra, Nipun and Kelly, Jack and Singh, Amarjeet and Knottenbelt, William and Rogers, Alex},
  Booktitle                = {1st International Symposium on Signal Processing Applications in Smart Buildings at 3rd IEEE Global Conference on Signal \& Information Processing (GlobalSIP'15)},
  Year                     = {2015},
  Eventdate                = {14-16 December 2015},
  Location                 = {Orlando, FL, USA},
  Month                    = {December},
  Organization             = {IEEE},

  __markedentry            = {[Jack:6]},
  Keywords                 = {energy, NILM, NILMTK},
  Owner                    = {Jack},
  Timestamp                = {2015.11.19}
}

@Inproceedings{pereira2015openEDF,
  Title                    = {Towards Systematic Performance Evaluation of Non-Intrusive Load Monitoring Algorithms and Systems},
  Author                   = {Pereira, Lucas and Nunes, Nuno J},
  Booktitle                = {Sustainable Internet and ICT for Sustainability (SustainIT), 2015},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {1--3},

  Abstract                 = {In this paper we present our approach to create an end-to-end software platform to enable the creation of meaningful and systematic, cross-dataset performance evaluations and benchmarks of Non-Intrusive Load Monitoring technology. We specifically propose a new file format to represent public datasets, a software framework to implement algorithms and metrics as well as the application of ceiling analysis to evaluate the overall performance of NILM systems.},
  File                     = {pereira2015openEDF.pdf:pereira2015openEDF.pdf:PDF},
  Keywords                 = {NILM, open-source, file format, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.05.13}
}

@Article{rezende2014stochastic,
  Title                    = {Stochastic backpropagation and approximate inference in deep generative models},
  Author                   = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  Year                     = {2014},

  File                     = {rezende2014stochastic.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/rezende2014stochastic.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1401.4082},
  Keywords                 = {DeepMind, generative},
  Owner                    = {jack},
  Timestamp                = {2015.03.30}
}

@Inproceedings{rifai2011contractive,
  Title                    = {Contractive auto-encoders: Explicit invariance during feature extraction},
  Author                   = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  Year                     = {2011},
  Pages                    = {833--840},

  Abstract                 = {We present in this paper a novel approach
for training deterministic auto-encoders. We
show that by adding a well chosen penalty
term to the classical reconstruction cost function, we can achieve results that equal or surpass those
attained by other regularized auto-encoders
as well as denoising auto-encoders
on a range of datasets. This penalty term
corresponds to the Frobenius norm of the
Jacobian matrix of the encoder activations
with respect to the input. We show that
this penalty term results in a localized space
contraction which in turn yields robust features
on the activation layer. Furthermore, we show how this penalty term is related to
both regularized auto-encoders and denoising
auto-encoders and how it can be seen as a link
between deterministic and non-deterministic
auto-encoders. We nd empirically that this
penalty helps to carve a representation that
better captures the local directions of variation
dictated by the data, corresponding to a
lower-dimensional non-linear manifold, while
being more invariant to the vast majority of
directions orthogonal to the manifold.
Finally, we show that by using the learned
features to initialize a MLP, we achieve state
of the art classication error on a range of
datasets, surpassing other methods of pre-training.},
  File                     = {rifai2011contractive-auto-encoders.pdf:rifai2011contractive-auto-encoders.pdf:PDF},
  Keywords                 = {autoencoder},
  Owner                    = {Jack},
  Timestamp                = {2015.05.01}
}

@Article{rolfe2013discriminative-recurrent-sparse-auto-encoders,
  Title                    = {Discriminative Recurrent Sparse Auto-Encoders},
  Author                   = {Rolfe, Jason Tyler and LeCun, Yann},
  Year                     = {2013},
  Eprint                   = {1301.3775},
  Eprinttype               = {arXiv},

  Abstract                 = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters.
From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
  File                     = {rolfe2013discriminative-recurrent-sparse-auto-encoders.pdf:rolfe2013discriminative-recurrent-sparse-auto-encoders.pdf:PDF},
  Keywords                 = {autoencoder, recurrent, discriminative},
  Owner                    = {Jack},
  Timestamp                = {2015.04.21}
}

@Inproceedings{roos1994neural-nets-nilm,
  Title                    = {Using neural networks for non-intrusive monitoring of industrial electrical loads},
  Author                   = {Roos, JG and Lane, IE and Botha, EC and Hancke, Gerhard P},
  Booktitle                = {Instrumentation and Measurement Technology Conference, 1994. IMTC/94. Conference Proceedings. 10th Anniversary. Advanced Technologies in I \& M., 1994 IEEE},
  Year                     = {1994},
  Doi                      = {10.1109/IMTC.1994.351862},
  Organization             = {IEEE},
  Pages                    = {1115--1118},

  Abstract                 = {The success of demand side energy control in industries, mines and commercial buildings depends on factors like the energy sensitivity and awareness of the organisation as well as an accurate and effective measurement and monitoring of its electrical energy consumption. Demand side energy control also forms an important part in the research programs of many research organisations. Reliable data on energy consumption is therefore imperative for effective research in this field, as well as for the successful implementation of demand side management. Traditional load research instrumentation has involved intrusive techniques that require the installation of sensors on each of the individual components of the total load. A non-intrusive appliance load monitor is proposed in this paper to determine the energy consumption of individual appliances turning on or off or operating under continuously varying load conditions. This monitoring system, which is implemented by network pattern identification technology, is based on detailed analysis of the current and voltage of the total load, as measured at the interface of the power source. The approach has been developed to simplify the collection of energy consumption data by utilities, but also has other applications. It is called nonintrusive to contrast it with previous techniques for gathering appliance data, which require placing sensors on individual appliances, and hence an intrusion onto the energy consumer's property.},
  Comment                  = {just a proposal!!!},
  File                     = {roos1994neural-nets-nilm.pdf:roos1994neural-nets-nilm.pdf:PDF},
  Keywords                 = {NILM, neural networks, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Techreport{rumelhart1985learning,
  Title                    = {Learning internal representations by error propagation},
  Author                   = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  Institution              = {DTIC Document},
  Year                     = {1985},

  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Misc{russakovsky14imagenet,
  Title                    = {ImageNet Large Scale Visual Recognition Challenge},
  Author                   = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  Year                     = {2014},
  Eprint                   = {1409.0575},
  Eprinttype               = {arXiv},

  File                     = {russakovsky14imagenet.pdf:russakovsky14imagenet.pdf:PDF},
  Keywords                 = {data, machine learning, image classification},
  Owner                    = {Jack},
  Timestamp                = {2014.11.12}
}

@Inproceedings{ruzzelli2010,
  Title                    = {Real-time recognition and profiling of appliances through a single electricity sensor},
  Author                   = {Ruzzelli, Antonio G and Nicolas, C and Schoofs, Anthony and O'Hare, Gregory MP},
  Booktitle                = {Sensor Mesh and Ad Hoc Communications and Networks (SECON), 2010 7th Annual IEEE Communications Society Conference on},
  Year                     = {2010},
  Doi                      = {10.1109/SECON.2010.5508244},
  Organization             = {IEEE},
  Pages                    = {1--9},

  Abstract                 = {Sensing, monitoring and actuating systems are expected to play a key role in reducing buildings overall energy consumption. Leveraging sensor systems to support energy efficiency in buildings poses novel research challenges in monitoring space usage, controlling devices, interfacing with smart energy meters and communicating with the energy grid. In the attempt of reducing electricity consumption in buildings, identifying individual sources of energy consumption is key to generate energy awareness and improve efficiency of available energy resources usage. Previous work studied several non-intrusive load monitoring techniques to classify appliances; however, the literature lacks of an comprehensive system that can be easily installed in existing buildings to empower users profiling, benchmarking and recognizing loads in real-time. This has been a major reason holding back the practice adoption of load monitoring techniques. In this paper we present RECAP: RECognition of electrical Appliances and Profiling in real-time. RECAP uses a single wireless energy monitoring sensor easily clipped to the main electrical unit. The energy monitoring unit transmits energy data wirelessly to a local machine for data processing and storage. The RECAP system consists of three parts: (1) Guiding the user for profiling electrical appliances within premises and generating a database of unique appliance signatures; (2) Using those signatures to train an artificial neural network that is then employed to recognize appliance activities (3) Providing a Load descriptor to allow peer appliance benchmarking. RECAP addresses the need of an integrated and intuitive tool to empower building owners with energy awareness. Enabling real-time appliance recognition is a stepping-stone towards reducing energy consumption and allowing a number of major applications including load-shifting techniques, energy expenditure breakdown per appliance, detection of power hungry and faulty appliances, and recogn- - ition of occupant activity. This paper describes the system design and performance evaluation in domestic environment.},
  File                     = {ruzzelli2010.pdf:ruzzelli2010.pdf:PDF},
  Keywords                 = {NILM, neural networks, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Article{sonderby2015RNN-SPN,
  Title                    = {Recurrent Spatial Transformer Networks},
  Author                   = {S{\o}nderby, S{\o}ren Kaae and S{\o}nderby, Casper Kaae and Maal{\o}e, Lars and Winther, Ole},
  Year                     = {2015},
  Eprint                   = {1509.05329},
  Eprinttype               = {arXiv},

  Abstract                 = {We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest.},
  Comment                  = {Related: jaderberg2015STN},
  File                     = {sonderby2015RNN-SPN.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/sonderby2015RNN-SPN.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1509.05329},
  Keywords                 = {deep learning, spatial transformer networks},
  Owner                    = {jack},
  Timestamp                = {2015.09.23}
}

@Article{pybrain2010jmlr,
  Title                    = {PyBrain},
  Author                   = {Schaul, Tom and Bayer, Justin and Wierstra, Daan and Sun, Yi and Felder, Martin and Sehnke, Frank and R{\"u}ckstie{\ss}, Thomas and Schmidhuber, J{\"u}rgen},
  Year                     = {2010},
  Pages                    = {743--746},
  Volume                   = {11},

  Journal                  = {Journal of Machine Learning Research},
  Keywords                 = {machine learning, software, open-source, LSTM}
}

@Article{schmidhuber2014deep-learning-overview,
  Title                    = {Deep Learning in Neural Networks: An Overview},
  Author                   = {J{\"u}rgen Schmidhuber},
  Year                     = {2014},
  Doi                      = {10.1016/j.neunet.2014.09.003},
  Eprint                   = {1404.7828},
  Month                    = {apr},
  Url                      = {http://people.idsia.ch/~juergen/deep-learning-overview.html},

  Abstract                 = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  Comments                 = {88 pages, 888 references},
  File                     = {:schmidhuber2014deep-learning-overview.pdf:PDF},
  Journal                  = {Neural Networks},
  Keywords                 = {deep learning, review},
  Oai2identifier           = {1404.7828},
  Owner                    = {jack},
  Reportno                 = {Technical Report IDSIA-03-14},
  Timestamp                = {2014.10.23}
}

@Article{sorrell2015reducing,
  Title                    = {Reducing energy demand: A review of issues, challenges and approaches},
  Author                   = {Steve Sorrell},
  Journaltitle             = {Renewable and Sustainable Energy Reviews},
  Year                     = {2015},
  Doi                      = {10.1016/j.rser.2015.03.002},
  ISSN                     = {1364-0321},
  Pages                    = {74 - 82},
  Volume                   = {47},

  Abstract                 = {Abstract Most commentators expect improved energy efficiency and reduced energy demand to provide the dominant contribution to tackling global climate change. But at the global level, the correlation between increased wealth and increased energy consumption is very strong and the impact of policies to reduce energy demand is both limited and contested. Different academic disciplines approach energy demand reduction in different ways: emphasising some mechanisms and neglecting others, being more or less optimistic about the potential for reducing energy demand and providing insights that are more or less useful for policymakers. This article provides an overview of the main issues and challenges associated with energy demand reduction, summarises how this challenge is ‘framed’ by key academic disciplines, indicates how these can provide complementary insights for policymakers and argues that a ‘sociotechnical’ perspective can provide a deeper understanding of the nature of this challenge and the processes through which it can be achieved. The article integrates ideas from the natural sciences, economics, psychology, innovation studies and sociology but does not give equal weight to each. It argues that reducing energy demand will prove more difficult than is commonly assumed and current approaches will be insufficient to deliver the transformation required.},
  File                     = {sorrell2015reducing.pdf:sorrell2015reducing.pdf:PDF},
  Keywords                 = {energy, psychology, energy demand reduction},
  Owner                    = {Jack},
  Timestamp                = {2015.06.24}
}

@Inproceedings{staudemeyer2013evaluating-LSTM,
  Title                    = {Evaluating Performance of Long Short-term Memory Recurrent Neural Networks on Intrusion Detection Data},
  Author                   = {Staudemeyer, Ralf C. and Omlin, Christian W.},
  Booktitle                = {Proceedings of the South African Institute for Computer Scientists and Information Technologists Conference},
  Year                     = {2013},
  Doi                      = {10.1145/2513456.2513490},
  ISBN                     = {978-1-4503-2112-9},
  Location                 = {East London, South Africa},
  Pages                    = {218--224},
  Publisher                = {ACM},
  Series                   = {SAICSIT '13},

  Acmid                    = {2513490},
  Address                  = {New York, NY, USA},
  Keywords                 = {KDDCup99, intrusion detection systems, long short-term memory, machine learning, receiver operating characteristic, recurrent neural networks, RNNs, LSTM},
  Numpages                 = {7},
  Owner                    = {Jack},
  Timestamp                = {2014.10.30}
}

@Inproceedings{sutskever2014machine-translation,
  Title                    = {Sequence to Sequence Learning with Neural Networks},
  Author                   = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  Booktitle                = {Advances in Neural Information Processing Systems 27},
  Editor                   = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
  Year                     = {2014},
  Eprint                   = {1409.3215},
  Eprinttype               = {arXiv},
  Pages                    = {3104--3112},
  Publisher                = {Curran Associates, Inc.},
  Pubstate                 = {forthcoming},

  Archiveprefix            = {arXiv},
  File                     = {:sutskever2014machine-translation.pdf:PDF},
  Keywords                 = {LSTM, language, machine translation},
  Notes                    = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks}
}

@Article{szegedy2014going-deeper,
  Title                    = {Going deeper with convolutions},
  Author                   = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  Year                     = {2014},
  Eprint                   = {1409.4842},
  Eprinttype               = {arXiv},

  File                     = {szegedy2014going-deeper.pdf:szegedy2014going-deeper.pdf:PDF},
  Keywords                 = {deep learning},
  Owner                    = {Jack},
  Timestamp                = {2014.11.12}
}

@Inproceedings{tokuda2000speech,
  Title                    = {Speech parameter generation algorithms for HMM-based speech synthesis},
  Author                   = {Tokuda, Keiichi and Yoshimura, Takayoshi and Masuko, Takashi and Kobayashi, Takao and Kitamura, Tadashi},
  Booktitle                = {Acoustics, Speech, and Signal Processing, 2000. ICASSP'00. Proceedings. 2000 IEEE International Conference on},
  Year                     = {2000},
  Doi                      = {10.1109/ICASSP.2000.861820},
  Organization             = {IEEE},
  Pages                    = {1315--1318},
  Volume                   = {3},

  File                     = {tokuda2000speech.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/tokuda2000speech.pdf:PDF},
  Keywords                 = {speech, HMMs},
  Owner                    = {jack},
  Timestamp                = {2015.03.20}
}

@Article{tsodyks2002STDP,
  Title                    = {Spike-timing-dependent synaptic plasticity--The long road towards understanding neuronal mechanisms of learning and memory},
  Author                   = {Tsodyks, Misha},
  Year                     = {2002},
  Number                   = {12},
  Pages                    = {599--600},
  Volume                   = {25},

  File                     = {tsodyks2002STDP.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/tsodyks2002STDP.pdf:PDF},
  Journal                  = {TRENDS in Neurosciences},
  Keywords                 = {neuroscience, STDP, synaptic plasticity},
  Owner                    = {jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.03.21}
}

@Article{uria2014density-estimator,
  Title                    = {A Deep and Tractable Density Estimator},
  Author                   = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {467--475},
  Volume                   = {32},

  File                     = {uria2014density-estimator.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/uria2014density-estimator.pdf:PDF},
  Journal                  = {JMLR: W\&CP},
  Keywords                 = {density estimation, deep learning},
  Owner                    = {jack},
  Timestamp                = {2015.03.20}
}

@Inproceedings{uria2012articulatory-inversion,
  Title                    = {Deep architectures for articulatory inversion},
  Author                   = {Benigno Uria and Iain Murray and Steve Renals and Korin Richmond},
  Booktitle                = {Proceedings of Interspeech},
  Year                     = {2012},

  File                     = {uria2012articulatory-inversion.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/uria2012articulatory-inversion.pdf:PDF},
  Keywords                 = {density estimation},
  Owner                    = {jack},
  Timestamp                = {2015.03.20}
}

@Article{uria2015trajectory-RNADE,
  Title                    = {Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE},
  Author                   = {Uria, Benigno and Murray, Iain and Renals, Steve and Valentini, Cassia and Bridle, John},
  Year                     = {2015},

  File                     = {uria2015trajectory-RNADE.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/uria2015trajectory-RNADE.pdf:PDF},
  Journal                  = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2015},
  Keywords                 = {density estimation},
  Owner                    = {jack},
  Timestamp                = {2015.03.20}
}

@Inproceedings{vincent2008denoising-autoencoders,
  Title                    = {Extracting and composing robust features with denoising autoencoders},
  Author                   = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  Booktitle                = {Proceedings of the 25th international conference on Machine learning},
  Year                     = {2008},
  Organization             = {ACM},
  Pages                    = {1096--1103},

  File                     = {vincent2008denoising-autoencoders.pdf:vincent2008denoising-autoencoders.pdf:PDF},
  Keywords                 = {neural networks, autoencoder},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Article{wahlstrom2014deep-dynamical,
  Title                    = {Learning deep dynamical models from image pixels},
  Author                   = {Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B and Deisenroth, Marc Peter},
  Year                     = {2014},
  Eprint                   = {1410.7550},
  Eprinttype               = {arXiv},

  File                     = {wahlstrom2014deep-dynamical.pdf:wahlstrom2014deep-dynamical.pdf:PDF},
  Journal                  = {arXiv preprint arXiv:1410.7550},
  Keywords                 = {machine learning, deep neural networks},
  Owner                    = {Jack},
  Timestamp                = {2015.04.09}
}

@Inproceedings{weninger2014RNN-de-reverb,
  Title                    = {Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition},
  Author                   = {Weninger, Felix and Watanabe, Shinji and Tachioka, Yuuki and Schuller, Bjorn},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {4623--4627},

  Abstract                 = {This paper describes our joint efforts to provide robust automatic
speech recognition (ASR) for reverberated environments, such as in
hands-free human-machine interaction. We investigate blind feature
space de-reverberation and deep recurrent de-noising auto-encoders
(DAE) in an early fusion scheme. Results on the 2014 REVERB
Challenge development set indicate that the DAE front-end provides
complementary performance gains to multi-condition training, feature
transformations, and model adaptation. The proposed ASR system
achieves word error rates of 17.62% and 36.6% on simulated and real
data, which is a significant improvement over the Challenge baseline
(25.16 and 47.2%)},
  File                     = {weninger2014RNN-de-reverb.pdf:weninger2014RNN-de-reverb.pdf:PDF},
  Keywords                 = {RNN, recurrent, speech, ASR, autoencoder, DRDAE},
  Owner                    = {Jack},
  Timestamp                = {2015.04.21}
}

@Article{werbos1990BPTT,
  Title                    = {Backpropagation through time: what it does and how to do it},
  Author                   = {Werbos, Paul J},
  Journaltitle             = {Proceedings of the IEEE},
  Year                     = {1990},
  Number                   = {10},
  Pages                    = {1550--1560},
  Volume                   = {78},

  Journal                  = {Proceedings of the {IEEE}},
  Keywords                 = {neural networks, recurrent neural networks},
  Owner                    = {Jack},
  Publisher                = {IEEE},
  Timestamp                = {2015.07.23}
}

@Article{werbos1988generalization,
  Title                    = {Generalization of backpropagation with application to a recurrent gas market model},
  Author                   = {Werbos, Paul J},
  Journaltitle             = {Neural Networks},
  Year                     = {1988},
  Number                   = {4},
  Pages                    = {339--356},
  Volume                   = {1},

  Journal                  = {Neural Networks},
  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Publisher                = {Elsevier},
  Timestamp                = {2015.07.23}
}

@Article{williams1995gradient,
  Title                    = {Gradient-based learning algorithms for recurrent networks and their computational complexity},
  Author                   = {Williams, Ronald J and Zipser, David},
  Journaltitle             = {Back-propagation: Theory, architectures and applications},
  Year                     = {1995},
  Pages                    = {433--486},

  Journal                  = {Back-propagation: Theory, architectures and applications},
  Keywords                 = {neural networks},
  Owner                    = {Jack},
  Timestamp                = {2015.07.23}
}

@Inproceedings{yang2007neural-net-nilm,
  Title                    = {Design a neural network for features selection in non-intrusive monitoring of industrial electrical loads},
  Author                   = {Yang, Hong-Tzer and Chang, Hsueh-Hsien and Lin, Ching-Lung},
  Booktitle                = {Computer Supported Cooperative Work in Design, 2007. CSCWD 2007. 11th International Conference on},
  Year                     = {2007},
  Doi                      = {10.1109/CSCWD.2007.4281579},
  Organization             = {IEEE},
  Pages                    = {1022--1027},

  Abstract                 = {This paper proposes to compare the performance of neural network classifiers between back propagation (BP) and learning vector quantization (LVQ) for pattern analyses of features selection in a non-intrusive load monitoring (NILM) system. Load recognition for identifying loads being connected and disconnected is applied to a NILM by using a neural network, especially for industrial electrical loads, even though some loads are activated at the nearly same time. In order to accurately decompose the aggregate load into its components, a feature-based model for describing the signatures of individual appliances and load combinations is used. The model will suggest the certain signatures which can be detected for all loads in order to indicate the activities of the separate components. To verify the performance of the model for the features selection, the data sets of the electrical loads and the load recognition techniques apply an electromagnetic transient program (EMTP) and a neural network, respectively. The effectiveness and computation equipment of load recognition are analyzed and compared by using the back propagation classifier and the learning vector quantization classifier. To obtain a maximum recognition accuracy rate, the calculation of the turn-on transient energy signature employs a window of samples, At, to adaptively segment a transient representative of a class of loads. Experiments performed with a variety of model data sets which reveal the back propagation classifier is superior to the learning quantization classifier in the effectiveness and computation equipment of load recognition.},
  File                     = {yang2007neural-net-nilm.pdf:yang2007neural-net-nilm.pdf:PDF},
  Keywords                 = {NILM, neural networks, energy},
  Owner                    = {Jack},
  Timestamp                = {2015.07.22}
}

@Article{zaremba2014execute,
  Title                    = {Learning to Execute},
  Author                   = {Wojciech Zaremba and Ilya Sutskever},
  Year                     = {2014},
  Eprint                   = {1410.4615},
  Eprinttype               = {arXiv},

  Archiveprefix            = {arXiv},
  File                     = {:zaremba2014execute.pdf:PDF},
  Keywords                 = {LSTM, Python},
  Owner                    = {jack},
  Timestamp                = {2014.10.22}
}

@Incollection{zeiler2014visualizing,
  Title                    = {Visualizing and understanding convolutional networks},
  Author                   = {Zeiler, Matthew D and Fergus, Rob},
  Booktitle                = {Computer Vision--ECCV 2014},
  Year                     = {2014},
  Eprint                   = {1311.2901},
  Eprinttype               = {arXiv},
  Pages                    = {818--833},
  Publisher                = {Springer},

  Abstract                 = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  File                     = {zeiler2014visualizing.pdf:/home/dk3810/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/zeiler2014visualizing.pdf:PDF},
  Keywords                 = {convolutional neural networks, deconvolutional, visualisation},
  Owner                    = {Jack},
  Timestamp                = {2015.05.01}
}

@comment{jabref-meta: selector_keywords:CNNs;deep learning;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 KeywordGroup:Energy\;2\;keywords\;energy\;0\;0\;;
2 KeywordGroup:NILM\;1\;keywords\;NILM\;0\;0\;;
2 KeywordGroup:Behaviour\;1\;keywords\;psychology|behaviour\;0\;1\;;
2 KeywordGroup:Neural Networks in Energy\;1\;keywords\;Neural Networks
\;0\;0\;;
1 KeywordGroup:Machine Learning\;2\;keywords\;[CNNs|convolutional neur
al networks|deconvolutional|deep learning|unsupervised learning|machin
e learning|deep neural networks|neural networks|genetic algorithm|mixt
ure density networks|density estimation|RNNs|deep reinforcement learni
ng]\;0\;1\;;
1 KeywordGroup:Neuroscience\;2\;keywords\;neuroscience\;0\;0\;;
1 ExplicitGroup:To read\;0\;sorrell2015reducing\;;
}

