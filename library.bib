@comment{jabref-meta: selector_keywords:CNNs;deep learning;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 KeywordGroup:Energy\;2\;keywords\;energy\;0\;0\;;
2 KeywordGroup:NILM\;1\;keywords\;NILM\;0\;0\;;
2 KeywordGroup:Behaviour\;1\;keywords\;psychology|behaviour\;0\;1\;;
2 KeywordGroup:Neural Networks in Energy\;1\;keywords\;Neural Networks
\;0\;0\;;
1 KeywordGroup:Machine Learning\;2\;keywords\;[CNNs|convolutional neur
al networks|deconvolutional|deep learning|unsupervised learning|machin
e learning|deep neural networks|neural networks|genetic algorithm|mixt
ure density networks|density estimation|RNNs|deep reinforcement learni
ng]\;0\;1\;;
1 KeywordGroup:Neuroscience\;2\;keywords\;neuroscience\;0\;0\;;
1 ExplicitGroup:To read\;0\;sorrell2015reducing\;;
}

@article{abbott2000synaptic-plasticity,
 author = {Abbott, Larry F and Nelson, Sacha B},
 file = {abbott2000synaptic-plasticity.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/abbott2000synaptic-plasticity.pdf:PDF},
 journal = {Nature neuroscience},
 keywords = {neuroscience, synaptic plasticity},
 owner = {jack},
 pages = {1178--1183},
 publisher = {Nature Publishing Group},
 timestamp = {2015.03.21},
 title = {Synaptic plasticity: taming the beast},
 volume = {3},
 year = {2000}
}

@article{adams2007changepoint-detection,
 abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
 author = {Adams, Ryan Prescott and MacKay, David JC},
 comment = {Python code is not available. Here's the MATLAB code: http://www.inference.phy.cam.ac.uk/rpa23/changepoint.php},
 eprint = {0710.3742},
 eprinttype = {arXiv},
 keywords = {machine learning, changepoint detection},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {Bayesian online changepoint detection},
 year = {2007}
}

@article{akbari1995disaggregate-hourly,
 abstract = {We have developed an algorithm to disaggregate short-interval (hourly) whole-building electrical load into major end uses. Hourly load data, hourly load-temperature regression coefficients and simulation end-use results comprise the algorithm input. The algorithm produces hourly load profiles for air conditioning, lighting, fans and pumps, and miscellaneous loads. Measured data from two end-use metered buildings (an office and a retail store) have been used to validate the algorithm. For the retail store, the algorithm estimates of hourly end use compare remarkably well with the monitored end-use data (average error of less than 5% during daytime operation). For the office building, the algorithm gives a consistent bias of about 12 and 27% in overestimating the HVAC and lighting electric loads, respectively, at the expense of underestimating the miscellaenous load by 35%. Results may be attributed to the presence of inconsistencies between office audit information and measured end-use data. A three-fold difference between the auditor's estimate for miscellaneous energy use and the metered amount has been found. The validation, however, indicates great promise for application of the algorithm to whole-building load data for obtaining reliable end-use data.},
 author = {Akbari, Hashem},
 doi = {10.1016/0360-5442(95)00033-D},
 journaltitle = {Energy},
 keywords = {NILM, energy, very low freq},
 number = {12},
 owner = {Jack},
 pages = {1291--1301},
 publisher = {Elsevier},
 timestamp = {2015.11.19},
 title = {Validation of an algorithm to disaggregate whole-building hourly electrical load into end uses},
 volume = {20},
 year = {1995}
}

@inproceedings{amirach2014feature-extraction,
 author = {Amirach, Nabil and Xerri, Bernard and Borloz, Bruno and Jauffret, Claude},
 booktitle = {Electronics, Circuits and Systems (ICECS), 2014 21st IEEE International Conference on},
 keywords = {NILM, feature extraction, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {287--290},
 timestamp = {2015.07.23},
 title = {A new approach for event detection and feature extraction for NILM},
 year = {2014}
}

@patent{amram2014historical,
 author = {Amram, Martha and Sauder, Doug},
 keywords = {NILM, energy},
 month = {5},
 number = {US2011282506},
 owner = {Jack},
 publisher = {Google Patents},
 timestamp = {2015.11.19},
 title = {Historical utility consumption disaggregation},
 url = {http://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=2011282506A1&KC=A1&FT=D&date=20111117&DB=EPODOC&locale=en_gb#},
 year = {2014}
}

@inproceedings{anderson2012blued,
 author = {Anderson, Kyle and Ocneanu, Adrian and Benitez, Diego and Carlson, Derrick and Rowe, Anthony and Berges, Mario},
 booktitle = {Proceedings of the 2nd KDD workshop on data mining applications in sustainability (SustKDD)},
 keywords = {NILM, energy, data},
 owner = {Jack},
 pages = {1--5},
 timestamp = {2015.11.19},
 title = {BLUED: A fully labeled public dataset for event-based non-intrusive load monitoring research},
 year = {2012}
}

@inproceedings{anderson2012event-detection,
 author = {Anderson, Kyle D and Berg{\'e}s, Mario E and Ocneanu, Adrian and Benitez, Diego and Moura, Jos{\'e} MP},
 booktitle = {IECON 2012-38th Annual Conference on IEEE Industrial Electronics Society},
 doi = {10.1109/IECON.2012.6389367},
 keywords = {NILM, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {3312--3317},
 timestamp = {2015.11.19},
 title = {Event detection for non intrusive load monitoring},
 year = {2012}
}

@inproceedings{arjunan2012sensoract,
 acmid = {2422547},
 address = {New York, NY, USA},
 author = {Arjunan, Pandarasamy and Batra, Nipun and Choi, Haksoo and Singh, Amarjeet and Singh, Pushpendra and Srivastava, Mani B.},
 booktitle = {Proceedings of the Fourth ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Buildings (BuildSys)},
 doi = {10.1145/2422531.2422547},
 isbn = {978-1-4503-1170-0},
 keywords = {actuators, architecture, building management, deployment, middleware, sensors, energy},
 location = {Toronto, Ontario, Canada},
 numpages = {8},
 owner = {Jack},
 pages = {80--87},
 publisher = {ACM},
 series = {BuildSys '12},
 timestamp = {2015.11.19},
 title = {SensorAct: A Privacy and Security Aware Federated Middleware for Building Management},
 year = {2012}
}

@article{armel2013holy-grail,
 abstract = {This paper aims to address two timely energy problems. First, significant low-cost energy reductions can be made in the residential and commercial sectors, but these savings have not been achievable to date. Second, billions of dollars are being spent to install smart meters, yet the energy saving and financial benefits of this infrastructure – without careful consideration of the human element – will not reach its full potential. We believe that we can address these problems by strategically marrying them, using disaggregation. Disaggregation refers to a set of statistical approaches for extracting end-use and/or appliance level data from an aggregate, or whole-building, energy signal. In this paper, we explain how appliance level data affords numerous benefits, and why using the algorithms in conjunction with smart meters is the most cost-effective and scalable solution for getting this data. We review disaggregation algorithms and their requirements, and evaluate the extent to which smart meters can meet those requirements. Research, technology, and policy recommendations are also outlined.},
 author = {Armel, K Carrie and Gupta, Abhay and Shrimali, Gireesh and Albert, Adrian},
 doi = {10.1016/j.enpol.2012.08.062},
 file = {armel2013holy-grail.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/armel2013holy-grail.pdf:PDF},
 journal = {Energy Policy},
 journaltitle = {Energy Policy},
 keywords = {NILM, energy},
 owner = {Jack},
 pages = {213--234},
 publisher = {Elsevier},
 timestamp = {2015.09.07},
 title = {Is disaggregation the holy grail of energy efficiency? The case of electricity},
 volume = {52},
 year = {2013}
}

@inproceedings{atlas1988artificial,
 author = {Atlas, Les E and Homma, Toshiteru and Marks II, Robert J},
 booktitle = {Proc. Neural Information Processing Systems (NIPS)},
 file = {atlas1988artificial.pdf:atlas1988artificial.pdf:PDF},
 keywords = {neural networks, convolutional},
 owner = {Jack},
 pages = {31},
 timestamp = {2015.07.23},
 title = {An artificial neural network for spatio-temporal bipolar patterns: Application to phoneme classification},
 year = {1988}
}

@article{auret2010changepoint-detection,
 abstract = {A large class of monitoring problems can be cast as the detection of a change in the parameters of a static or dynamic system, based on the effects of these changes on one or more observed variables. In this paper, the use of random forest models to detect change points in dynamic systems is considered. The approach is based on the embedding of multivariate time series data associated with normal process conditions, followed by the extraction of features from the resulting lagged trajectory matrix. The features are extracted by recasting the data into a binary classification problem, which can be solved with a random forest model. A proximity matrix can be calculated from the model and from this matrix features can be extracted that represent the trajectory of the system in phase space. The results of the study suggest that the random forest approach may afford distinct advantages over a previously proposed linear equivalent, particularly when complex nonlinear systems need to be monitored.},
 author = {Auret, Lidia and Aldrich, Chris},
 doi = {10.1016/j.conengprac.2010.04.005},
 journal = {Control Engineering Practice},
 keywords = {changepoint detection, machine learning},
 number = {8},
 owner = {Jack},
 pages = {990--1002},
 publisher = {Elsevier},
 timestamp = {2015.11.19},
 title = {Change point detection in time series data with random forests},
 volume = {18},
 year = {2010}
}

@patent{ayachitula2010influencing,
 abstract = {A method for performing utility consumption disaggregation includes measuring a total utility consumption of a consumer during a specified time period, generating a first disaggregated utility consumption segment and a second disaggregated utility consumption segment, based on the total utility consumption of the consumer, and providing the consumer with disaggregated utility consumption statistics based on at least one of the first and second disaggregated utility consumption segments.},
 author = {Ayachitula, Naga A and Chao, Tian-Jy and Dai, Jing D and Naphade, Milind R and Sahu, Sambit},
 keywords = {NILM},
 number = {US20100880915 20100913},
 owner = {Jack},
 publisher = {Google Patents},
 timestamp = {2015.11.19},
 title = {Influencing Consumer Behavior Modification with Utility Consumption Disaggregation},
 url = {http://worldwide.espacenet.com/publicationDetails/biblio?FT=D&date=20110317&DB=EPODOC&locale=en_gb&CC=US&NR=2011066442A1&KC=A1},
 year = {2010}
}

@article{babaei2015dataset-directory,
 abstract = {Energy consumption data are required to perform analysis, modelling, evaluation, and optimisation of energy usage in buildings. While a variety of energy consumption data sets have been examined and reported in the literature, there is a lack of a comprehensive categorisation and analysis of the available data sets. In this study, an overview of energy consumption data of buildings is provided. Three common strategies for generating energy consumption data, i.e., measurement, survey, and simulation, are described. A number of important characteristics pertaining to each strategy and the resulting data sets are discussed. In addition, a directory of energy consumption data sets of buildings is developed. The data sets are collected from either published papers or energy related organisations. The main contributions of this study include establishing a resource pertaining to energy consumption data sets and providing information related to the characteristics and availability of the respective data sets; therefore facilitating and promoting research activities in energy consumption data analysis.},
 author = {Babaei, Toktam and Abdi, Hamid and Lim, Chee Peng and Nahavandi, Saeid},
 doi = {10.1016/j.enbuild.2015.02.043},
 journal = {Energy and Buildings},
 keywords = {energy, datasets, NILM},
 owner = {Jack},
 pages = {91--99},
 publisher = {Elsevier},
 timestamp = {2015.04.17},
 title = {A study and a directory of energy consumption data sets of buildings},
 volume = {94},
 year = {2015}
}

@article{balan2011parameter,
 abstract = {HVAC (Heating, Ventilation and Air Conditioning) systems used for heating or cooling buildings, consume a considerable amount of energy. To optimize the energy consumption, the behavior of occupants must be changed. This can be achieved by providing information and suggestions to occupants. A first step is developing of a less expensive and non-invasive measurement system and metering of the electricity and heat consumed. Based on collected experimental data, it can identify the parameters of a thermal model of the house. The model obtained will be used to simulate different aspects that can help to reduce the energy consumption. This paper presents a simple solution for thermal modeling of a house which includes experimental identification of the model's parameters. Such data are used to simulate the thermal behavior of the house and to obtain solutions to reduce energy consumption. In simulation, the control of the thermal system is performed using a model based predictive control algorithm.},
 author = {B{\u{a}}lan, Radu and Cooper, Joshua and Chao, Kuo-Ming and Stan, Sergiu and Donca, Radu},
 doi = {10.1016/j.enbuild.2010.10.023},
 journaltitle = {Energy and Buildings},
 keywords = {energy, heat},
 number = {2},
 owner = {Jack},
 pages = {748--758},
 publisher = {Elsevier},
 timestamp = {2015.11.19},
 title = {Parameter identification and model based predictive control of temperature inside a house},
 volume = {43},
 year = {2011}
}

@inproceedings{baranski2004dynamic-programming,
 abstract = {Nonintrusive appliance load monitoring (NIALM) systems require sufficient accurate total load data to separate the load into its major appliances. The most available solutions separate the whole electric energy consumption based on the measurement of all three voltages and currents. Aside from the cost for special measuring devices, the intrusion into the local installation is the main problem for reaching a high market distribution. The use of standard digital electricity meters could avoid this problem but the loss of information of the measured data has to be compensated by more intelligent algorithms and implemented rules to disaggregate the total load trace of only the active power measurements. The paper presents a NIALM approach to analyse data, collected from a standard digital electricity meter. To disaggregate the consumption of the entire active power into its major electrical end uses, an algorithm consisting of clustering methods, a genetic algorithm and a dynamic programming approach is presented. The genetic algorithm is used to combine frequently occurring events to create hypothetical finite state machines to model detectable appliances. The time series of each finite state machine is optimized using a dynamic programming method similar to the viterbi algorithm.},
 author = {Baranski, Michael and Voss, J{\"u}rgen},
 booktitle = {Fourth IEEE International Conference on Data Mining (ICDM'04)},
 doi = {10.1109/ICDM.2004.10003},
 keywords = {energy, NILM, dynamic programming},
 organization = {IEEE},
 owner = {Jack},
 pages = {327--330},
 timestamp = {2015.11.19},
 title = {Detecting patterns of appliances from total load data using a dynamic programming approach},
 year = {2004}
}

@inproceedings{baranski2004genetic-algorithm,
 author = {Baranski, Michael and Voss, J{\"u}rgen},
 booktitle = {Systems, Man and Cybernetics, 2004 IEEE International Conference on},
 doi = {10.1109/ICSMC.2004.1400878},
 keywords = {energy, NILM, genetic algorithm},
 organization = {IEEE},
 owner = {Jack},
 pages = {3462--3468},
 timestamp = {2015.11.19},
 title = {Genetic algorithm for pattern detection in NIALM systems},
 volume = {4},
 year = {2004}
}

@book{barber2011bayesian,
 author = {Barber, David and Cemgil, A Taylan and Chiappa, Silvia},
 isbn = {0521196760},
 keywords = {machine learning, bayesian},
 owner = {Jack},
 publisher = {Cambridge University Press},
 timestamp = {2015.11.19},
 title = {Bayesian time series models},
 year = {2011}
}

@book{barber2012bayesian,
 abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
 author = {Barber, David},
 isbn = {9780521518147},
 keywords = {machine learning, bayesian},
 owner = {Jack},
 publisher = {Cambridge University Press},
 timestamp = {2015.11.19},
 title = {Bayesian reasoning and machine learning},
 url = {http://www.cs.ucl.ac.uk/staff/d.barber/brml/},
 year = {2012}
}

@inproceedings{barker2012smart,
 author = {Barker, Sean and Mishra, Aditya and Irwin, David and Cecchet, Emmanuel and Shenoy, Prashant and Albrecht, Jeannie},
 booktitle = {The 1st KDD Workshop on Data Mining Applications in Sustainability (SustKDD)},
 keywords = {NILM, data},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {Smart*: An open data set and tools for enabling research in sustainable homes},
 year = {2012}
}

@inproceedings{barker2013electrical-loads,
 abstract = {Smart meter deployments are spurring renewed interest in analysis techniques for electricity usage data. An important prerequisite for data analysis is characterizing and modeling how electrical loads use power. While prior work has made significant progress in deriving insights from electricity data, one issue that limits accuracy is the use of general and often simplistic load models. Prior models often associate a fixed power level with an “on” state and either no power, or some minimal amount, with an “off” state. This paper’s goal is to develop a new methodology for modeling electric loads that is both simple and accurate. Our approach is empirical in nature: we monitor a wide variety of common loads to distill a small number of common usage characteristics, which we leverage to construct accurate load-specific models. We show that our models are significantly more accurate than binary on-off models, decreasing the root mean square error by as much as 8X for representative loads. Finally, we demonstrate two example uses of our models in data analysis: i) generating device-accurate synthetic traces of building electricity usage, and ii) filtering out loads that generate rapid and random power variations in building electricity data},
 author = {Barker, Scott and Kalra, Sandeep and Irwin, David and Shenoy, Prashant},
 booktitle = {International Green Computing Conference (IGCC)},
 keywords = {energy, electrical, simulation, appliances},
 organization = {IEEE},
 owner = {Jack},
 pages = {1--10},
 timestamp = {2015.11.19},
 title = {Empirical characterization and modeling of electrical loads in smart homes},
 year = {2013}
}

@article{barry1993bayesian,
 author = {Barry, Daniel and Hartigan, John A},
 journaltitle = {Journal of the American Statistical Association},
 keywords = {machine learning, changepoint detection},
 number = {421},
 owner = {Jack},
 pages = {309--319},
 publisher = {Taylor \& Francis},
 timestamp = {2015.11.19},
 title = {A Bayesian analysis for change point problems},
 url = {http://www.jstor.org/stable/2290726},
 volume = {88},
 year = {1993}
}

@misc{bastien2012theano,
 abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.},
 author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
 eprint = {1211.5590},
 eprinttype = {arXiv},
 howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
 keywords = {neural networks},
 owner = {Jack},
 timestamp = {2015.07.23},
 title = {Theano: new features and speed improvements},
 year = {2012}
}

@inproceedings{batra2013indic,
 __markedentry = {[Jack:]},
 address = {Miami, FL, USA},
 author = {Batra, Nipun and Dutta, Haimonti and Singh, Amarjeet},
 booktitle = {12th International Conference on Machine Learning and Applications (ICMLA)},
 keywords = {energy, NILM},
 organization = {IEEE},
 owner = {Jack},
 pages = {79--84},
 timestamp = {2015.11.19},
 title = {INDiC: Improved Non-Intrusive load monitoring using load Division and Calibration},
 volume = {1},
 year = {2013}
}

@inproceedings{batra2013occupancy,
 abstract = {Buildings are one of the largest consumers of electricity. Dominant electricity consumption within the buildings, contributed by plug loads, lighting and air conditioning, can be significantly improved using Occupancy-based Building Management Systems (Ob-BMS). In this paper, we address three critical aspects of Ob-BMS i.e. 1) Modular sensor node design to support diverse deployment scenarios; 2) Building architecture to support and scale fine resolution monitoring; and 3) Detailed analysis of the collected data for smarter actuation. We present key learning across these three aspects evolved over more than one year of design and deployment experiences. The sensor node design evolved over a period of time to address specific deployment requirements. With an opportunity at the host institute where two dorm buildings were getting constructed, we planned for the support infrastructure required for fine resolution monitoring embedded in the design phase and share our preliminary experiences and key learning thereof. Prototype deployment of the sensing system as per the planned support infrastructure was performed at two faculty offices with effective data collection worth 45 days. Collected data is analyzed accounting for efficient switching of appliances, in addition to energy conservation and user comfort as performed in the earlier occupancy based frameworks. Our analysis shows that occupancy prediction using simple heuristic based modeling can achieve similar performance as more complex Hidden Markov Models, thus simplifying the analytic framework.},
 author = {Batra, Nipun and Arjunan, Pandarasamy and Singh, Ashutosh and Singh, Prashant},
 booktitle = {IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)},
 doi = {10.1109/ISSNIP.2013.6529781},
 keywords = {energy, building management systems},
 organization = {IEEE},
 owner = {Jack},
 pages = {153--158},
 timestamp = {2015.11.19},
 title = {Experiences with Occupancy based Building Management Systems},
 year = {2013}
}

@article{batra2014comparison,
 author = {Batra, Nipun and Parson, Oliver and Berges, Mario and Singh, Amarjeet and Rogers, Alex},
 eprint = {1408.6595},
 eprinttype = {arXiv},
 keywords = {energy, NILM},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {A comparison of non-intrusive load monitoring methods for commercial and residential buildings},
 year = {2014}
}

@inproceedings{batra2014plc,
 author = {Batra, Nipun and Gulati, Manoj and Jain, Puneet and Whitehouse, Kamin and Singh, Amarjeet},
 booktitle = {Proceedings of the First ACM International Conference on Embedded Systems For Energy-Efficient Buildings (BuildSys)},
 keywords = {energy, NILM, PLC},
 organization = {ACM},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {Bits and Watts: Improving energy disaggregation performance using power line communication modems},
 year = {2014}
}

@thesis{batra2014qualifier-report,
 address = {IIIT Delhi},
 author = {Batra, Nipun and Singh, Amarjeet and Singh, Pushpendra and Dutta, Haimonti and Sarangan, Venkatesh and Srivastava Srivastava, Mani},
 booktitle = {PhD qualifier report},
 eprint = {1404.7227},
 eprinttype = {arXiv},
 institution = {IIIT Delhi},
 keywords = {energy},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {Data Driven Energy Efficiency in Buildings},
 type = {PhD qualifier report},
 year = {2014}
}

@inproceedings{Batra:2013:DIH:2528282.2528293,
 acmid = {2528293},
 address = {New York, NY, USA},
 articleno = {3},
 author = {Batra, Nipun and Gulati, Manoj and Singh, Amarjeet and Srivastava, Mani B.},
 booktitle = {Proceedings of the 5th ACM Workshop on Embedded Systems For Energy-Efficient Buildings (BuildSys)},
 doi = {10.1145/2528282.2528293},
 isbn = {978-1-4503-2431-1},
 keywords = {energy, sensor networks, smart homes, deployment, buildings},
 location = {Roma, Italy},
 numpages = {8},
 owner = {Jack},
 pages = {3:1--3:8},
 publisher = {ACM},
 series = {BuildSys'13},
 timestamp = {2015.11.19},
 title = {It's Different: Insights into Home Energy Consumption in India},
 year = {2013}
}

@article{bengio2007scaling,
 author = {Bengio, Yoshua and LeCun, Yann and others},
 journal = {Large-scale kernel machines},
 journaltitle = {Large-scale kernel machines},
 keywords = {neural networks},
 number = {5},
 owner = {Jack},
 timestamp = {2015.07.23},
 title = {Scaling learning algorithms towards {AI}},
 volume = {34},
 year = {2007}
}

@article{bengio2015biologically-plausible,
 abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
 author = {Bengio, Yoshua and Dong-Hyun, Lee and Bornschein, Jorg and Lin, Zhouhan},
 eprint = {1502.04156},
 eprinttype = {arXiv},
 file = {bengio2015biologically-plausible.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/bengio2015biologically-plausible.pdf:PDF},
 keywords = {deep learning},
 owner = {jack},
 timestamp = {2015.03.21},
 title = {Towards Biologically Plausible Deep Learning},
 year = {2015}
}

@unpublished{bengio2015book,
 author = {Yoshua Bengio and Ian J. Goodfellow and Aaron Courville},
 keywords = {machine learning, deep learning},
 note = {Book in preparation for MIT Press},
 owner = {jack},
 timestamp = {2015.03.30},
 title = {Deep Learning},
 url = {http://www.iro.umontreal.ca/~bengioy/dlbook},
 year = {2015}
}

@article{benitti2012robotics-in-schools,
 abstract = {This study reviews recently published scientific literature on the use of robotics in schools, in order to: (a) identify the potential contribution of the incorporation of robotics as educational tool in schools, (b) present a synthesis of the available empirical evidence on the educational effectiveness of robotics as an educational tool in schools, and (c) define future research perspectives concerning educational robotics. After systematically searching online bibliographic databases, ten relevant articles were located and included in the study. For each article, we analyze the purpose of the study, the content to be taught with the aid of robotics, the type of robot used, the research method used, and the sample characteristics (sample size, age range of students and/or level of education) and the results observed. The articles reviewed suggest that educational robotics usually acts as an element that enhances learning, however, this is not always the case, as there are studies that have reported situations in which there was no improvement in learning. The outcomes of the literature review are discussed in terms of their implications for future research, and can provide useful guidance for educators, practitioners and researchers in the area.},
 author = {Benitti, Fabiane Barreto Vavassori},
 doi = {10.1016/j.compedu.2011.10.006},
 file = {benitti2012robotics-in-schools.pdf:benitti2012robotics-in-schools.pdf:PDF},
 journal = {Computers \& Education},
 journaltitle = {Computers & Education},
 keywords = {education, robotics},
 number = {3},
 owner = {Jack},
 pages = {978--988},
 publisher = {Elsevier},
 timestamp = {2015.10.16},
 title = {Exploring the educational potential of robotics in schools: A systematic review},
 volume = {58},
 year = {2012}
}

@inproceedings{benyoucef2010,
 abstract = {One feature of Smart Homes includes recording the consumption of electricity of appliances in the household. The recordings are conducted using Smart Meters. To demonstrate the usefulness of Smart Meters for the final customers, this paper identifies several benefits by presenting the research area non-intrusive appliance load monitoring (NALM). The purpose is to discuss the energy consumption of different appliances in detail in order to determine a means of reducing this consumption. The analysis focuses on the drawbacks of the appliances examined. Based on the results of the analysis, the approach to gathering data is described. With this approach we want to greatly simplify the NALM process. Finally the initial results obtained using this approach are discussed.},
 author = {Benyoucef, D. and Klein, P. and Bier, T.},
 booktitle = {IEEE International Energy Conference and Exhibition (EnergyCon)},
 doi = {10.1109/ENERGYCON.2010.5771810},
 file = {benyoucef2010.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/benyoucef2010.pdf:PDF},
 isbn = {978-1-4244-9378-4},
 keywords = {energy, NILM},
 language = {English},
 month = {Dec},
 owner = {jack},
 pages = {96--101},
 publisher = {IEEE},
 timestamp = {2015.11.20},
 title = {Smart Meter with non-intrusive load monitoring for use in Smart Homes},
 year = {2010}
}

@inproceedings{bergstra2010theano,
 abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
 author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
 booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
 keywords = {machine learning, software, open-source, deep learning},
 location = {Austin, TX},
 month = {6},
 note = {Oral Presentation},
 owner = {Jack},
 timestamp = {2015.07.23},
 title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
 year = {2010}
}

@article{bers2014computational-thinking,
 abstract = {By engaging in construction-based robotics activities, children as young as four can play to learn a range of concepts. The TangibleK Robotics Program paired developmentally appropriate computer programming and robotics tools with a constructionist curriculum designed to engage kindergarten children in learning computational thinking, robotics, programming, and problem-solving. This paper documents three kindergarten classrooms' exposure to computer programming concepts and explores learning outcomes. Results point to strengths of the curriculum and areas where further redesign of the curriculum and technologies would be appropriate. Overall, the study demonstrates that kindergartners were both interested in and able to learn many aspects of robotics, programming, and computational thinking with the TangibleK curriculum design.},
 author = {Bers, Marina Umaschi and Flannery, Louise and Kazakoff, Elizabeth R and Sullivan, Amanda},
 doi = {10.1016/j.compedu.2013.10.020},
 file = {bers2014computational-thinking.pdf:bers2014computational-thinking.pdf:PDF},
 journal = {Computers \& Education},
 journaltitle = {Computers & Education},
 keywords = {education, robotics},
 owner = {Jack},
 pages = {145--157},
 publisher = {Elsevier},
 timestamp = {2015.10.16},
 title = {Computational thinking and tinkering: Exploration of an early childhood robotics curriculum},
 volume = {72},
 year = {2014}
}

@book{bishop2006PRML,
 author = {Christopher M. Bishop},
 keywords = {machine learning},
 owner = {Jack},
 publisher = {springer New York},
 timestamp = {2014.10.27},
 title = {Pattern recognition and machine learning},
 volume = {1},
 year = {2006}
}

@techreport{Bishop94mixturedensity,
 author = {Christopher M. Bishop},
 institution = {Aston University, Birmingham, UK},
 keywords = {density estimation},
 owner = {Jack},
 timestamp = {2014.10.27},
 title = {Mixture density networks},
 url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.120.5685},
 year = {1994}
}

@inproceedings{boulanger2012music-generation,
 author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
 booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
 eprint = {1206.6392},
 eprinttype = {arXiv},
 file = {boulanger2012music-generation.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/boulanger2012music-generation.pdf:PDF},
 keywords = {RNN-RBM, RNNs, generative, RBMs},
 owner = {jack},
 timestamp = {2015.03.30},
 title = {Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription},
 year = {2012}
}

@article{buchanan2015energy-reduction,
 author = {Buchanan, Kathryn and Russo, Riccardo and Anderson, Ben},
 doi = {10.1016/j.enpol.2014.12.008},
 journal = {Energy Policy},
 keywords = {energy, psychology, smart meters, user engagement},
 owner = {jack},
 pages = {89--96},
 publisher = {Elsevier},
 timestamp = {2015.03.26},
 title = {The question of energy reduction: The problem (s) with feedback},
 volume = {77},
 year = {2015}
}

@article{bull2015moving-beyond-feedback,
 abstract = {The energy savings potential within non-domestic buildings from behaviour change initiatives is well known. Energy efficiency measures can contribute to local, national and EU policy commitments on carbon reduction. Yet, research also shows behaviour change is anything but simple. No-where is this more evident that in local government where municipalities are expected to lead on carbon reduction initiatives whilst operating in challenging political landscapes. This paper reflects on a UK Research Council funded case study exploring the role of engagement in a UK municipality. Innovative feedback tools and user-engagement were developed in an effort to foster a collaborative approach to energy management.

Findings from an analysis of a focus group and a set of semi-structured interviews show encouraging signs with regard to increased user-engagement and digital tools, but barriers remain with regards to the ‘real world’ implementation of innovative, and technologically grounded, approaches. These included a staff reduction programme amidst financial cuts, a risk-averse culture with regard to new technologies, and debate about where responsibilities lie with regards to energy management. While these findings were case specific they have implications for organisations contemplating how technology might support them in workplace engagement for reduced energy use.},
 author = {Bull, Richard and Lemon, Mark and Everitt, Dave and Stuart, Graeme},
 doi = {10.1016/j.erss.2015.04.006},
 journal = {Energy Research \& Social Science},
 keywords = {energy, feedback, behaviour},
 owner = {Jack},
 pages = {32--40},
 publisher = {Elsevier},
 timestamp = {2015.06.11},
 title = {Moving beyond feedback: Energy behaviour and local engagement in the United Kingdom},
 volume = {8},
 year = {2015}
}

@article{chandola2009anomaly-detection-survey,
 acmid = {1541882},
 address = {New York, NY, USA},
 articleno = {15},
 author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
 doi = {10.1145/1541880.1541882},
 issn = {0360-0300},
 issue_date = {July 2009},
 journal = {ACM Comput. Surv.},
 keywords = {Anomaly detection, outlier detection},
 month = {Jul},
 number = {3},
 numpages = {58},
 owner = {Jack},
 pages = {15:1--15:58},
 publisher = {ACM},
 timestamp = {2014.10.30},
 title = {Anomaly Detection: A Survey},
 volume = {41},
 year = {2009}
}

@inproceedings{chang2011feature-extraction,
 author = {Chang, Hsueh-Hsien and Chien, Po-Ching and Lin, Lung-Shu and Chen, Nanming},
 booktitle = {e-Business Engineering (ICEBE), 2011 IEEE 8th International Conference on},
 keywords = {NILM, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {299--304},
 timestamp = {2015.07.23},
 title = {Feature extraction of non-intrusive load-monitoring system using genetic algorithm in smart meters},
 year = {2011}
}

@article{chorowski2014end-to-end-speech,
 author = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
 eprint = {1412.1602},
 eprinttype = {arXiv},
 file = {chorowski2014end-to-end-speech.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/chorowski2014end-to-end-speech.pdf:PDF},
 keywords = {machine learning, deep learning, speech, attention},
 owner = {jack},
 timestamp = {2015.03.30},
 title = {End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results},
 year = {2014}
}

@article{chung2015gated,
 abstract = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
 author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
 eprint = {1502.02367},
 eprinttype = {arXiv},
 file = {chung2015gated.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/chung2015gated.pdf:PDF},
 keywords = {RNNs},
 owner = {jack},
 timestamp = {2015.03.21},
 title = {Gated Feedback Recurrent Neural Networks},
 year = {2015}
}

@article{cichon2015dendritic-spikes,
 abstract = {The brain has an extraordinary capacity for memory storage, but how it stores new information without disrupting previously acquired memories remains unknown. Here we show that different motor learning tasks induce dendritic Ca2+ spikes on different apical tuft branches of individual layer V pyramidal neurons in the mouse motor cortex. These task-related, branch-specific Ca2+ spikes cause long-lasting potentiation of postsynaptic dendritic spines active at the time of spike generation. When somatostatin-expressing interneurons are inactivated, different motor tasks frequently induce Ca2+ spikes on the same branches. On those branches, spines potentiated during one task are depotentiated when they are active seconds before Ca2+ spikes induced by another task. Concomitantly, increased neuronal activity and performance improvement after learning one task are disrupted when another task is learned. These findings indicate that dendritic-branch-specific generation of Ca2+ spikes is crucial for establishing long-lasting synaptic plasticity, thereby facilitating information storage associated with different learning experiences.},
 author = {Cichon, Joseph and Gan, Wen-Biao},
 date = {2015/03/30},
 doi = {10.1038/nature14251},
 file = {cichon2015dendritic-spikes.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/cichon2015dendritic-spikes.pdf:PDF},
 journal = {Nature},
 journaltitle = {Nature},
 keywords = {neuroscience, synaptic plasticity},
 owner = {jack},
 publisher = {Nature Publishing Group},
 timestamp = {2015.04.08},
 title = {Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity},
 year = {2015}
}

@inproceedings{fabius2015variational,
 abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
 author = {Fabius, Otto and van Amersfoort, Joost R and Kingma, Diederik P},
 booktitle = {ICLR workshop track},
 eprint = {1412.6581},
 eprinttype = {arXiv},
 file = {fabius2015variational-recurrent-AE.pdf:fabius2015variational-recurrent-AE.pdf:PDF},
 keywords = {variational, autoencoder, recurrent},
 owner = {Jack},
 timestamp = {2015.04.21},
 title = {Variational Recurrent Auto-Encoders},
 year = {2015}
}

@article{felder2010wind,
 author = {Felder, Martin and Kaifel, Anton and Graves, Alex},
 file = {felder2010wind.pdf:felder2010wind.pdf:PDF},
 journal = {Poster Presentation at the European Wind Energy Conference},
 keywords = {mixture density networks, density estimation, RNNs},
 owner = {jack},
 timestamp = {2014.10.22},
 title = {Wind power prediction using mixture density recurrent neural networks},
 url = {http://proceedings.ewea.org/ewec2010/allfiles2/413_EWEC2010presentation.pdf},
 year = {2010}
}

@article{fukushima1980neocognitron,
 author = {Fukushima, Kunihiko},
 file = {fukushima1980neocognitron.pdf:fukushima1980neocognitron.pdf:PDF},
 journal = {Biological cybernetics},
 journaltitle = {Biological cybernetics},
 keywords = {neural networks, convolutional},
 number = {4},
 owner = {Jack},
 pages = {193--202},
 publisher = {Springer},
 timestamp = {2015.07.23},
 title = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
 volume = {36},
 year = {1980}
}

@article{germain2015MADE,
 author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
 file = {germain2015MADE.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/germain2015MADE.pdf:PDF},
 keywords = {density estimation},
 owner = {jack},
 timestamp = {2015.03.20},
 title = {MADE: Masked Autoencoder for Distribution Estimation},
 year = {2015}
}

@article{Gomez-Marin2014big-behavioural-data,
 author = {Gomez-Marin, Alex and Paton, Joseph J and Kampff, Adam R and Costa, Rui M and Mainen, Zachary F},
 doi = {10.1038/nn.3812},
 issn = {1097-6256},
 journal = {Nature Neuroscience},
 keywords = {neuroscience, automatic-neuroscientist},
 language = {en},
 month = {Oct},
 number = {11},
 pages = {1455--1462},
 publisher = {Nature Publishing Group},
 title = {Big behavioral data: psychology, ethology and the foundations of neuroscience},
 url = {http://www.nature.com/neuro/journal/v17/n11/full/nn.3812.html?WT.ec\_id=NEURO-201411},
 volume = {17},
 year = {2014}
}

@inproceedings{goroshin2014unsupervised-feature-learning-from-temporal-data,
 author = {Goroshin, Ross and Bruna, Joan and Szlam, Arthur and Tompson, Jonathan and Eigen, David and LeCun, Yann},
 booktitle = {Deep Learning and Representation Learning Workshop, NIPS},
 comment = {Related: goroshin2015unsupervised-feature-learning-from-temporal-data},
 file = {goroshin2014unsupervised-feature-learning-from-temporal-data.pdf:goroshin2014unsupervised-feature-learning-from-temporal-data.pdf:PDF},
 keywords = {unsupervised learning, CNNs, timeseries, video},
 owner = {Jack},
 timestamp = {2015.04.15},
 title = {Unsupervised Feature Learning from Temporal Data},
 year = {2014}
}

@article{goroshin2015unsupervised-feature-learning-from-temporal-data,
 author = {Goroshin, Ross and Bruna, Joan and Tompson, Jonathan and Eigen, David and LeCun, Yann},
 comment = {Related: goroshin2015unsupervised-feature-learning-from-temporal-data},
 eprint = {1504.02518},
 eprinttype = {arXiv},
 file = {goroshin2015unsupervised-feature-learning-from-temporal-data.pdf:goroshin2015unsupervised-feature-learning-from-temporal-data.pdf:PDF},
 keywords = {unsupervised learning, CNNs, timeseries, video, machine learning},
 owner = {Jack},
 timestamp = {2015.04.15},
 title = {Unsupervised Feature Learning from Temporal Data},
 year = {2015}
}

@article{graves2008CTC-unconstrained-handwriting-recognition,
 abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
 author = {Graves, Alex and Liwicki, Marcus and Fern{\'a}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\"u}rgen},
 doi = {10.1109/TPAMI.2008.137},
 file = {:graves2008CTC-unconstrained-handwriting-recognition.pdf:PDF},
 journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
 keywords = {LSTM, handwriting, language},
 number = {5},
 owner = {jack},
 pages = {855--868},
 publisher = {IEEE},
 timestamp = {2014.10.22},
 title = {A novel connectionist system for unconstrained handwriting recognition},
 volume = {31},
 year = {2008}
}

@inproceedings{graves2012arabic-handwriting-recognition,
 author = {Alex Graves},
 booktitle = {Guide to OCR for Arabic Scripts},
 file = {:graves2012arabic-handwriting-recognition.pdf:PDF},
 keywords = {LSTM, handwriting, language},
 owner = {jack},
 pages = {297--313},
 publisher = {Springer London},
 timestamp = {2014.10.22},
 title = {Offline arabic handwriting recognition with multidimensional recurrent neural networks},
 year = {2012}
}

@book{graves2012book-supervised-sequence-labelling,
 author = {Alex Graves},
 file = {:graves2012book-supervised-sequence-labelling.pdf:PDF},
 keywords = {LSTM, RNNs},
 owner = {jack},
 publisher = {Springer},
 timestamp = {2014.10.22},
 title = {Supervised sequence labelling with recurrent neural networks},
 url = {http://www.cs.toronto.edu/~graves/preprint.pdf},
 volume = {385},
 year = {2012}
}

@article{graves2013generating-sequences,
 author = {Alex Graves},
 date = {2013/8/4},
 eprint = {1308.0850},
 eprinttype = {arXiv},
 file = {:graves2013generating-sequences.pdf:PDF},
 keywords = {LSTM},
 owner = {jack},
 timestamp = {2014.10.22},
 title = {Generating sequences with recurrent neural networks},
 year = {2013}
}

@inproceedings{graves2013speech-recognition-DBLSTM,
 author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
 booktitle = {Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on},
 doi = {10.1109/ASRU.2013.6707742},
 eventdate = {8-12 Dec. 2013},
 file = {:graves2013speech-recognition-DBLSTM.pdf:PDF},
 keywords = {LSTM, speech, language},
 location = {Olomouc},
 owner = {jack},
 pages = {273--278},
 timestamp = {2014.10.22},
 title = {Hybrid speech recognition with deep bidirectional LSTM},
 year = {2013}
}

@inproceedings{graves2014end-to-end-speech-recognition,
 author = {Alex Graves and Navdeep Jaitly},
 booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
 file = {:graves2014end-to-end-speech-recognition.pdf:PDF},
 keywords = {LSTM, RNNs, speech},
 notes = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2014c2_graves14.pdf},
 owner = {jack},
 pages = {1764-1772},
 timestamp = {2014.10.22},
 title = {Towards end-to-end speech recognition with recurrent neural networks},
 year = {2014}
}

@article{graves2014NTM,
 abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
 archiveprefix = {arXiv},
 author = {Alex Graves and Greg Wayne and Ivo Danihelka},
 eprint = {1410.5401},
 eprinttype = {arXiv},
 keywords = {LSTM, DeepMind},
 owner = {jack},
 timestamp = {2014.10.28},
 title = {Neural Turing Machines},
 year = {2014}
}

@article{gregor2015DRAW,
 author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
 eprint = {1502.04623},
 eprinttype = {arXiv},
 file = {gregor2015DRAW.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/gregor2015DRAW.pdf:PDF},
 keywords = {RNNs, generative, DeepMind, variational autoencoder},
 owner = {jack},
 timestamp = {2015.04.01},
 title = {DRAW: A Recurrent Neural Network For Image Generation},
 year = {2015}
}

@techreport{hart1985,
 author = {George William Hart},
 file = {hart1985.pdf:hart1985.pdf:PDF},
 institution = {MIT Energy Laboratory and Electric Power Research Institute},
 keywords = {NILM, energy},
 month = {sep},
 owner = {Jack},
 timestamp = {2015.07.22},
 title = {Prototype Nonintrusive Appliance Load Monitor},
 year = {1985}
}

@patent{hart1989,
 abstract = {A non-intrusive monitor of energy consumption of residential appliances is described in which sensors, coupled to the power circuits entering a residence, supply analog voltage and current signals which are converted to digital format and processed to detect changes in certain residential load parameters, i.e., admittance. Cluster analysis techniques are employed to group change measurements into certain categories, and logic is applied to identify individual appliances and the energy consumed by each.},
 author = {George W. Hart and Edward C. Kern and Fred C. Schweppe},
 keywords = {NILM, energy},
 number = {US4858141 A},
 owner = {Jack},
 timestamp = {2015.07.22},
 title = {Non-intrusive appliance monitor},
 url = {http://www.google.com/patents?vid=4858141},
 year = {1989}
}

@article{hart1989energy-monitoring,
 abstract = {The author notes that all novel technologies have the potential to affect society in a complex manner, with both beneficial and detrimental consequences. He considers an illustrative case study: a nonintrusive appliance load monitoring technique that can provide vital information to help avoid future energy crises, but can also be used for surveillance purposes. He notes that there appears to be a significant potential for the technology to be abused. The danger that the technology might eventually lead to an erosion of civil liberties and privacy rights leaves its developers in an ethical quandary. The author examines how this technology should be controlled},
 author = {George William Hart},
 doi = {10.1109/44.31557},
 file = {hart1989energy-monitoring.pdf:hart1989energy-monitoring.pdf:PDF},
 issn = {0278-0097},
 journal = {IEEE Technology and Society Magazine},
 journaltitle = {IEEE Technology and Society Magazine},
 keywords = {NILM, energy},
 language = {English},
 month = {jun},
 number = {2},
 owner = {Jack},
 pages = {12--16},
 timestamp = {2015.07.22},
 title = {Residential energy monitoring and computerized surveillance via utility power flows},
 url = {http://ieeexplore.ieee.org/ielx5/44/1367/00031557.pdf?tp=\&arnumber=31557\&isnumber=1367},
 volume = {8},
 year = {1989}
}

@article{hart1992,
 abstract = {A nonintrusive appliance load monitor that determines the energy consumption of individual appliances turning on and off in an electric load, based on detailed analysis of the current and voltage of the total load, as measured at the interface to the power source is described. The theory and current practice of nonintrusive appliance load monitoring are discussed, including goals, applications, load models, appliance signatures, algorithms, prototypes field-test results, current research directions, and the advantages and disadvantages of this approach relative to intrusive monitoring},
 author = {George William Hart},
 doi = {10.1109/5.192069},
 file = {hart1992.pdf:hart1992.pdf:PDF},
 issn = {00189219},
 journal = {Proceedings of the IEEE},
 journaltitle = {Proceedings of the IEEE},
 keywords = {NILM, energy},
 language = {English},
 month = {dec},
 number = {12},
 owner = {Jack},
 pages = {1870--1891},
 publisher = {IEEE},
 timestamp = {2015.07.22},
 title = {Nonintrusive appliance load monitoring},
 volume = {80},
 year = {1992}
}

@book{hart1995,
 author = {George William Hart},
 keywords = {NILM, energy},
 language = {en},
 owner = {Jack},
 publisher = {Electric Power Research Institute},
 timestamp = {2015.07.22},
 title = {Nonintrusive appliance load monitoring with finite-state appliance models},
 url = {http://books.google.co.uk/books?id=OeJfMQAACAAJ},
 year = {1995}
}

@incollection{hermans2013analysing-RNNs,
 author = {Hermans, Michiel and Schrauwen, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems 26},
 editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 file = {hermans2013analysing-RNNs.pdf:hermans2013analysing-RNNs.pdf:PDF},
 keywords = {RNNs},
 owner = {Jack},
 pages = {190--198},
 publisher = {Curran Associates, Inc.},
 timestamp = {2015.03.25},
 title = {Training and Analysing Deep Recurrent Neural Networks},
 url = {http://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks.pdf},
 year = {2013}
}

@article{hinton2006fast,
 author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
 journal = {Neural computation},
 journaltitle = {Neural computation},
 keywords = {neural networks},
 number = {7},
 owner = {Jack},
 pages = {1527--1554},
 publisher = {MIT Press},
 timestamp = {2015.07.23},
 title = {A fast learning algorithm for deep belief nets},
 volume = {18},
 year = {2006}
}

@article{hinton2006reducing,
 abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
 author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
 doi = {10.1126/science.1127647},
 file = {hinton2006reducing-dimensionality.pdf:hinton2006reducing-dimensionality.pdf:PDF},
 journal = {Science},
 keywords = {machine learning, autoencoder, RBMs,},
 number = {5786},
 owner = {Jack},
 pages = {504--507},
 publisher = {American Association for the Advancement of Science},
 timestamp = {2015.04.09},
 title = {Reducing the dimensionality of data with neural networks},
 volume = {313},
 year = {2006}
}

@inproceedings{hinton2014distilling,
 abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
 author = {Hinton, Geoffrey E and Vinyals, Oriol and Dean, Jeff},
 booktitle = {NIPS 2014 Deep Learning Workshop},
 eprint = {1503.02531},
 eprinttype = {arXiv},
 file = {hinton2014distilling.pdf:hinton2014distilling.pdf:PDF},
 keywords = {neural networks},
 owner = {Jack},
 timestamp = {2015.05.01},
 title = {Distilling the knowledge in a neural network},
 year = {2014}
}

@article{hochreiter1997LSTM,
 author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
 doi = {10.1162/neco.1997.9.8.1735},
 file = {:hochreiter1997LSTM.pdf:PDF},
 journal = {Neural Computation},
 journaltitle = {Neural Computation},
 keywords = {LSTM},
 number = {8},
 owner = {jack},
 pages = {1735--1780},
 publisher = {MIT Press},
 timestamp = {2014.10.22},
 title = {Long short-term memory},
 volume = {9},
 year = {1997}
}

@article{ioffe2015batch-normalization,
 abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
 author = {Ioffe, Sergey and Szegedy, Christian},
 eprint = {1502.03167},
 eprinttype = {arXiv},
 file = {ioffe2015batch-normalization.pdf:ioffe2015batch-normalization.pdf:PDF},
 keywords = {deep learning},
 owner = {Jack},
 timestamp = {2015.04.23},
 title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
 year = {2015}
}

@article{jaderberg2015STN,
 abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
 author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
 comment = {Related: sonderby2015RNN-SPN},
 eprint = {1506.02025},
 eprinttype = {arXiv},
 file = {jaderberg2015STN.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/jaderberg2015STN.pdf:PDF},
 keywords = {deep learning, spatial transformer networks},
 owner = {jack},
 timestamp = {2015.09.23},
 title = {Spatial Transformer Networks},
 year = {2015}
}

@phdthesis{jaitly2014thesis,
 author = {Jaitly, Navdeep},
 file = {jaitly2014thesis.pdf:jaitly2014thesis.pdf:PDF},
 institution = {University of Toronto},
 keywords = {deep learning, RBMs, speech, HMMs, capsules},
 owner = {Jack},
 timestamp = {2015.03.30},
 title = {Exploring Deep Learning Methods for discovering features in speech signals.},
 year = {2014}
}

@techreport{jeffery2015climate,
 abstract = {Aggregate INDC emissions are far above levels consistent with below
2°C, with around 65% of global emissions covered
INDCs announced by 1 September 2015 lead to global emissions far above the levels
needed by 2025 and 2030 to put the world on track to hold warming below 2°C, or to below
1.5°C, in 2100.
As of 1st September, 29 INDC submissions have been received, reflecting 56 countries
(including the European Union member states), and covering around 65% of global
emissions in 2010 (excluding LULUCF) and 43% of global population. The CAT has directly
assessed 16 of these INDCs covering 64.5% of global emissions in 2010 (excluding LULUCF)
and 41% of global population.
With the INDCs submitted to date, the CAT projects total global emissions are on track to
be 53-57 GtCO2e in 2025 and 55-59 GtCO2e in 2030, far above the least-cost global
pathways consistent with limiting warming below 2°C. Additional reductions in the order of
12-15 GtO2e by 2025 and of 17-21 GtCO2e by 2030 are needed for global emissions to be
consistent with a 2°C pathway.
INDCs are yet to come from 140 countries. The ten highest emitters yet to submit INDCs
are India, Brazil, Iran, Indonesia, Saudi Arabia, South Africa, Thailand, Turkey, Ukraine, and
Pakistan, together accounting for 18% of global emissions not yet covered by INDCs
(excluding LULUCF).
Aside from the insufficient ambition of the aggregate INDCs, there is a significant gap
between current policies and the INDCs: global emissions under currently implemented
policies are projected to be higher than the already inadequate INDC levels. Some countries
propose INDCs close to the current trajectory giving confidence that they are met (e.g. EU
and China). Others have put forward a target that would be a significant change in trend, but these are not yet supported by any significant existing legislation, e.g. Australia and
Canada, raising questions about the likely implementation. Yet others are showing progress
in policy implementation, continuously moving their future trajectories downwards, but
policies are not yet sufficient to meet their (still inadequate) INDCs (e.g. USA).
The gap between pledges and policies increases through time, highlighting the need for
long-term policy action. This is not to underplay the significance and importance of
governments putting in place policies that will actually reduce their emissions, but for many
governments this is not yet the case.},
 author = {Louise Jeffery and Ryan Alexander and Bill Hare and Marcia Rocha and Michiel Schaeffer and Niklas Höhne and Hanna Fekete and Pieter van Breevoort and Kornelis Blok},
 date = {1/09/2015},
 institution = {Climate Action Tracker},
 keywords = {climate change, environment, carbon emissions},
 owner = {jack},
 subtitle = {Climate Action Tracker Update},
 timestamp = {2015.09.07},
 title = {How close are INDCs to 2 and 1.5°C pathways?},
 url = {http://climateactiontracker.org/assets/publications/briefing_papers/CAT_EmissionsGap_Briefing_Sep2015.pdf},
 year = {2015}
}

@thesis{kelly2011msc-thesis,
 author = {Jack Kelly},
 institution = {Imperial College London},
 month = {September},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {Disaggregating Smart Meter Readings using Device Signatures},
 type = {Computing MSc Thesis},
 url = {http://www.doc.ic.ac.uk/teaching/distinguished-projects/2011/d.kelly.pdf},
 year = {2011}
}

@inproceedings{kelly2014metadata,
 address = {V{\" a}ster{\aa}s, Sweden},
 archiveprefix = {arXiv},
 arxivid = {1403.5946},
 author = {Kelly, Jack and Knottenbelt, William},
 booktitle = {Computer Software and Applications Conference Workshop (COMPSACW) at the 2nd IEEE International Workshop on Consumer Devices and Systems (CDS)},
 doi = {10.1109/COMPSACW.2014.97},
 eprint = {1403.5946},
 file = {kelly2014metadata.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/kelly2014metadata.pdf:PDF},
 keywords = {NILM, metadata, energy},
 owner = {jack},
 timestamp = {2015.05.19},
 title = {Metadata for Energy Disaggregation},
 year = {2014}
}

@inproceedings{kelly2014NILMTKv02,
 address = {Memphis, USA},
 author = {Kelly, Jack and Batra, Nipun and Parson, Oliver and Dutta, Haimonti and Knottenbelt, William and Rogers, Alex and Singh, Amarjeet and Srivastava, Mani},
 booktitle = {Proceedings of the First ACM International Conference on Embedded Systems For Energy-Efficient Buildings (BuildSys)},
 doi = {10.1145/2674061.2675024},
 eprint = {1409.5908},
 eprinttype = {arXiv},
 file = {kelly2014NILMTKv02.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/kelly2014NILMTKv02.pdf:PDF},
 keywords = {NILM, NILMTK, energy},
 location = {Memphis, TN, USA},
 organization = {ACM},
 owner = {Jack},
 timestamp = {2014.11.07},
 title = {NILMTK v0.2: A Non-intrusive Load Monitoring Toolkit for Large Scale Data Sets},
 year = {2014}
}

@article{kingma2013VAE,
 abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
 author = {Kingma, Diederik P and Welling, Max},
 file = {kingma2013VAE.pdf:kingma2013VAE.pdf:PDF},
 keywords = {autoencoder, variational},
 owner = {Jack},
 timestamp = {2015.05.21},
 title = {Auto-encoding variational bayes},
 year = {2013}
}

@inproceedings{kingma2014semi-supervised,
 author = {Kingma, Diederik P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems 27},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
 file = {kingma2014semi-supervised.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/kingma2014semi-supervised.pdf:PDF},
 keywords = {DeepMind, generative, semi-supervised},
 owner = {jack},
 pages = {3581--3589},
 publisher = {Curran Associates, Inc.},
 timestamp = {2015.03.30},
 title = {Semi-supervised Learning with Deep Generative Models},
 url = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf},
 year = {2014}
}

@inproceedings{kiros2014unifying,
 author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S},
 booktitle = {NIPS 2014 deep learning workshop},
 eprint = {1411.2539},
 eprinttype = {arXiv},
 keywords = {LSTM},
 owner = {jack},
 timestamp = {2014.11.17},
 title = {Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models},
 year = {2014}
}

@inproceedings{kolter2011REDD,
 author = {Kolter, J Zico and Johnson, Matthew J},
 booktitle = {Workshop on Data Mining Applications in Sustainability (SIGKDD), San Diego, CA},
 keywords = {energy, data, NILM},
 organization = {Citeseer},
 owner = {Jack},
 pages = {59--62},
 timestamp = {2015.07.23},
 title = {{REDD}: A public data set for energy disaggregation research},
 volume = {25},
 year = {2011}
}

@article{koutnik2014clockwork,
 author = {Koutn{\'\i}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
 eprint = {1402.3511},
 eprinttype = {arXiv},
 file = {koutnik2014clockwork.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/koutnik2014clockwork.pdf:PDF},
 keywords = {RNNs},
 owner = {jack},
 timestamp = {2015.03.31},
 title = {A Clockwork RNN},
 year = {2014}
}

@inproceedings{Koutnik2014evolving,
 address = {New York, New York, USA},
 author = {Koutn{\'\i}k, Jan and Schmidhuber, J{\"u}rgen and Gomez, Faustino},
 booktitle = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
 doi = {10.1145/2576768.2598358},
 isbn = {9781450326629},
 keywords = {deep learning,games,neuroevolution,reinforcement learning,vision-based torcs},
 month = {Jul},
 owner = {Jack},
 pages = {541--548},
 publisher = {ACM Press},
 timestamp = {2014.10.28},
 title = {Evolving deep unsupervised convolutional networks for vision-based reinforcement learning},
 url = {http://dl.acm.org/citation.cfm?id=2576768.2598358},
 year = {2014}
}

@inproceedings{krizhevsky2012imagenet,
 author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
 booktitle = {Advances in Neural Information Processing Systems 25},
 editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
 file = {krizhevsky2012imagenet.pdf:krizhevsky2012imagenet.pdf:PDF},
 keywords = {machine learning, CNNs, image classification, deep learning},
 owner = {Jack},
 pages = {1097--1105},
 publisher = {Curran Associates, Inc.},
 timestamp = {2014.11.12},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 year = {2012}
}

@article{larochelle2011NADE,
 author = {Hugo Larochelle and Iain Murray},
 file = {larochelle2011NADE.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/larochelle2011NADE.pdf:PDF},
 journal = {JMLR: W\&CP},
 keywords = {density estimation},
 owner = {jack},
 pages = {29--37},
 timestamp = {2015.03.20},
 title = {The Neural Autoregressive Distribution Estimator},
 volume = {15},
 year = {2011}
}

@article{le2015ReLU-RNNs,
 abstract = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
 author = {Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
 eprint = {1504.00941},
 eprinttype = {arXiv},
 file = {le2015ReLU-RNNs.pdf:le2015ReLU-RNNs.pdf:PDF},
 keywords = {RNNs, ReLU, LSTM, initialisation},
 owner = {Jack},
 timestamp = {2015.04.23},
 title = {A Simple Way to Initialize Recurrent Networks of Rectified Linear Units},
 year = {2015}
}

@article{lecun1998gradient,
 author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
 journal = {Proceedings of the IEEE},
 keywords = {neural networks, convolutional},
 number = {11},
 owner = {Jack},
 pages = {2278--2324},
 publisher = {IEEE},
 timestamp = {2015.07.23},
 title = {Gradient-based learning applied to document recognition},
 volume = {86},
 year = {1998}
}

@inproceedings{lee2009unsupervised,
 abstract = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. For the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
 author = {Lee, Honglak and Pham, Peter and Largman, Yan and Ng, Andrew Y},
 booktitle = {Advances in neural information processing systems},
 file = {lee2009unsupervised.pdf:lee2009unsupervised.pdf:PDF},
 keywords = {deep belief networks, audio, classification, unsupervised},
 owner = {Jack},
 pages = {1096--1104},
 timestamp = {2015.04.17},
 title = {Unsupervised feature learning for audio classification using convolutional deep belief networks},
 year = {2009}
}

@article{leeb1995transient,
 abstract = {This paper describes the theoretical foundation and prototype implementation of a power system transient event detector for use in a nonintrusive load monitor (NILM). The NILM determines the operating schedule of the major electrical loads in a building from measurements made at the electric utility service entry. The transient event detector extends the applicability of the NILM to challenging commercial and industrial sites. A spectral preprocessor for use in the transient event detector is introduced first. Then, the transient event detection algorithm is developed. The performance of the algorithm is illustrated with results from a prototype event detector},
 author = {Leeb, Steven B and Shaw, Steven R and Kirtley Jr, JJames L},
 doi = {10.1109/61.400897},
 file = {leeb1995transient.pdf:leeb1995transient.pdf:PDF},
 journal = {Power Delivery, IEEE Transactions on},
 journaltitle = {IEEE Transactions on Power Delivery},
 keywords = {NILM, feature extraction, event detection, energy},
 number = {3},
 owner = {Jack},
 pages = {1200--1210},
 publisher = {IEEE},
 timestamp = {2015.07.23},
 title = {Transient event detection in spectral envelope estimates for nonintrusive load monitoring},
 volume = {10},
 year = {1995}
}

@inproceedings{lin2010feature-extraction,
 abstract = {The novel feature extraction method for nonintrusive load monitoring (NILM) systems was proposed in this paper. In order to monitoring the status of each load, a sensor is installed for each load traditionally. The status of the load is transmitted to the main controller such that the status of each load can be monitored. On the other hand, the NILM is able to detect the status of loads by analyzing the current signals that is picked-up by a current sensor installed at the main electrical panel. Traditional NILM methods used real power, reactive power, harmonic contents of power signatures and transient energy to determine the status of appliances. These methods are very complex and require a lot of computation. In this paper, a novel method that integrates artificial intelligent recognition technique and load current acquisition method for NILM is proposed. The proposed method uses timedomain information. This approach is different from traditional NILM methods. The proposed method is able to detect the energization and de-energization of loads by applying back-propagation neural networks (BP-ANNs). The overall correct rate for this method is above 98.75%. This result shows that the proposed method is able to determine the operation status of loads with proper robustness.},
 author = {Lin, Yu-Hsiu and Tsai, Men-Shen},
 booktitle = {2010 International Symposium on Computer Communication Control and Automation (3CA)},
 doi = {10.1109/3CA.2010.5533571},
 file = {lin2010feature-extraction.pdf:lin2010feature-extraction.pdf:PDF},
 keywords = {NILM, neural networks, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {215--218},
 timestamp = {2015.07.22},
 title = {A novel feature extraction method for the development of nonintrusive load monitoring system based on {BP-ANN}},
 volume = {2},
 year = {2010}
}

@inproceedings{lowe1999SIFT,
 author = {Lowe, David G},
 booktitle = {Computer vision, 1999. The proceedings of the seventh {IEEE} international conference on},
 file = {lowe1999SIFT.pdf:lowe1999SIFT.pdf:PDF},
 keywords = {computer vision, feature detection},
 organization = {IEEE},
 owner = {Jack},
 pages = {1150--1157},
 timestamp = {2015.07.22},
 title = {Object recognition from local scale-invariant features},
 volume = {2},
 year = {1999}
}

@inproceedings{maas2012RNN-noise-reduction-in-ASR,
 abstract = {Recent work on deep neural networks as acoustic mod-
els for automatic speech recognition (ASR) have demon-
strated substantial performance improvements. We intro-
duce a model which uses a deep recurrent auto encoder
neural network to denoise input features for robust ASR.
The model is trained on stereo (noisy and clean) audio
features to predict clean features given noisy input. The
model makes no assumptions about how noise affects the
signal, nor the existence of distinct noise environments.
Instead, the model can learn to model any type of distor-
tion or additive noise given sufficient training data. We
demonstrate the model is competitive with existing fea-
ture denoising approaches on the Aurora2 task, and out-
performs a tandem approach where deep networks are
used to predict phoneme posteriors directly},
 author = {Maas, Andrew L and Le, Quoc V and O'Neil, Tyler M and Vinyals, Oriol and Nguyen, Patrick and Ng, Andrew Y},
 booktitle = {INTERSPEECH},
 comment = {Related: maas2013RNN-feature-enhancement},
 file = {maas2012RNN-noise-reduction-in-ASR.pdf:maas2012RNN-noise-reduction-in-ASR.pdf:PDF},
 keywords = {RNN, recurrent, speech, ASR, autoencoder, DRDAE},
 organization = {Citeseer},
 owner = {Jack},
 timestamp = {2015.04.21},
 title = {Recurrent Neural Networks for Noise Reduction in Robust ASR},
 url = {http://www1.icsi.berkeley.edu/~vinyals/Files/rnn_denoise_2012.pdf},
 year = {2012}
}

@inproceedings{maas2013RNN-feature-enhancement,
 abstract = {We apply a machine learning approach to improve noisy
acoustic features for robust speech recognition. Specifically, we train a deep, recurrent neural network to map noise-
corrupted input features to their corresponding clean ver-
sions. We introduce several improvements to previously pro-
posed neural network feature enhancement architectures. The
model does not include assumptions about the specific noise
and distortions present in CHiME data, but does assume noisy
and clean stereo pairs are available for training. When used
with the standard recognizer on the small vocabulary task
(track 1), our approach demonstrates substantial improve-
ments over the challenge baseline.},
 author = {Maas, Andrew L and O'Neil, Tyler M and Hannun, Awni Y and Ng, Andrew Y},
 booktitle = {Proceedings The 2nd CHiME Workshop on Machine Listening in Multisource Environments held in conjunction with ICASSP},
 comment = {Related: maas2012RNN-noise-reduction-in-ASR},
 file = {maas2013RNN-feature-enhancement.pdf:maas2013RNN-feature-enhancement.pdf:PDF},
 keywords = {RNN, recurrent, speech, ASR, autoencoder, DRDAE},
 owner = {Jack},
 pages = {79--80},
 timestamp = {2015.04.21},
 title = {Recurrent neural network feature enhancement: The 2nd CHiME challenge},
 url = {http://web.stanford.edu/~awni/papers/drdae_chime2013_final.pdf},
 year = {2013}
}

@inproceedings{makriyiannis2014argumentation,
 author = {Makriyiannis, Menelaos and Lung, Tudor and Craven, Robert and Toni, Francesca and Kelly, Jack},
 booktitle = {4th International Workshop on Combinations of Intelligent Methods and Applications in conjunction with the IEEE International Conference on Tools with AI},
 file = {makriyiannis2014argumentation.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/makriyiannis2014argumentation.pdf:PDF},
 keywords = {energy, recommender systems},
 owner = {jack},
 timestamp = {2015.05.19},
 title = {Smarter Electricity through Argumentation},
 year = {2014}
}

@incollection{masci2011stacked,
 abstract = {We present a novel convolutional auto-encoder (CAE) for
unsupervised feature learning. A stack of CAEs forms a convolutional
neural network (CNN). Each CAE is trained using conventional on-line
gradient descent without additional regularization terms. A max-pooling
layer is essential to learn biologically plausible features consistent with
those found by previous approaches. Initializing a CNN with filters of a
trained CAE stack yields superior performance on a digit (MNIST) and
an object recognition (CIFAR10) benchmark.},
 author = {Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
 booktitle = {Artificial Neural Networks and Machine Learning--ICANN 2011},
 file = {masci2011stacked-convolutional-auto-encoders.pdf:masci2011stacked-convolutional-auto-encoders.pdf:PDF},
 keywords = {autoencoder, convolutional},
 owner = {Jack},
 pages = {52--59},
 publisher = {Springer},
 timestamp = {2015.05.01},
 title = {Stacked convolutional auto-encoders for hierarchical feature extraction},
 year = {2011}
}

@techreport{mayhorn2015characteristics,
 author = {Mayhorn, E. T. and Butner, R.S. and Baechler, M. C. and Sullivan, G.P. and Hao, H.},
 file = {mayhorn2015characteristics.pdf:mayhorn2015characteristics.pdf:PDF},
 institution = {Pacific Northwest National Laboratory (PNNL), Richland, WA (US)},
 keywords = {NILM, energy},
 owner = {Jack},
 timestamp = {2015.11.09},
 title = {Characteristics and Performance of Existing Load Disaggregation Technologies},
 url = {http://www.pnnl.gov/main/publications/external/technical_reports/PNNL-24230.pdf},
 year = {2015}
}

@inproceedings{michalski2014grammar-cells,
 author = {Michalski, Vincent and Memisevic, Roland and Konda, Kishore},
 booktitle = {Advances in Neural Information Processing Systems (NIPS 2014)},
 eprint = {1402.2333},
 eprinttype = {arXiv},
 file = {michalski2014grammar-cells.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/michalski2014grammar-cells.pdf:PDF},
 keywords = {RNNs},
 owner = {jack},
 pages = {1925--1933},
 timestamp = {2015.04.01},
 title = {Modeling Deep Temporal Dependencies with Recurrent "Grammar Cells"},
 url = {http://www.iro.umontreal.ca/~memisevr/pubs/predictive2014.pdf},
 year = {2014}
}

@article{mnih2013playing-atari,
 author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
 eprint = {1312.5602},
 eprinttype = {arXiv},
 file = {:mnih2013playing-atari.pdf:PDF},
 keywords = {DeepMind, deep learning},
 owner = {jack},
 timestamp = {2014.10.22},
 title = {Playing Atari with deep reinforcement learning},
 year = {2013}
}

@article{mnih2015deep-reinforcement-learning,
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
 journal = {Nature},
 journaltitle = {Nature},
 keywords = {machine learning, deep learning, deep reinforcement learning},
 number = {7540},
 owner = {Jack},
 pages = {529--533},
 publisher = {Nature Publishing Group},
 timestamp = {2015.07.23},
 title = {Human-level control through deep reinforcement learning},
 volume = {518},
 year = {2015}
}

@inproceedings{NeuralNILM,
 author = {Jack Kelly and William Knottenbelt},
 booktitle = {Proceedings of the Second ACM Workshop On Embedded Systems For Energy-Efficient Buildings (BuildSys)},
 doi = {10.1145/2821650.2821672},
 eprint = {1507.06594},
 eprinttype = {arXiv},
 keywords = {energy, NILM, neural networks, deep learning},
 location = {Seoul, South Korea},
 month = {November},
 organization = {ACM},
 owner = {Jack},
 timestamp = {2015.09.07},
 title = {Neural NILM: Deep Neural Networks Applied to Energy Disaggregation},
 year = {2015}
}

@inproceedings{NILMTK,
 address = {Cambridge, UK},
 author = {Batra, Nipun and Kelly, Jack and Parson, Oliver and Dutta, Haimonti and Knottenbelt, William and Rogers, Alex and Singh, Amarjeet and Srivastava, Mani},
 booktitle = {Fifth International Conference on Future Energy Systems (ACM e-Energy)},
 doi = {10.1145/2602044.2602051},
 file = {NILMTK.pdf:NILMTK.pdf:PDF},
 keywords = {NILM, energy, NILMTK},
 owner = {jack},
 timestamp = {2015.05.19},
 title = {NILMTK: An Open Source Toolkit for Non-intrusive Load Monitoring},
 year = {2014}
}

@www{nouri2014facial-keypoints,
 author = {Daniel Nouri},
 keywords = {deep learning, vision, regression},
 owner = {jack},
 timestamp = {2015.07.18},
 title = {Using convolutional neural nets to detect facial keypoints tutorial},
 url = {http://bit.ly/1OduG83},
 year = {2014}
}

@article{olazaran1996perceptrons-controversy,
 author = {Olazaran, Mikel},
 file = {olazaran1996perceptrons-controversy.pdf:olazaran1996perceptrons-controversy.pdf:PDF},
 journal = {Social Studies of Science},
 keywords = {history, perceptrons},
 number = {3},
 owner = {Jack},
 pages = {611--659},
 publisher = {Sage Publications},
 timestamp = {2015.04.01},
 title = {A sociological study of the official history of the perceptrons controversy},
 url = {http://www.jstor.org/stable/285702},
 volume = {26},
 year = {1996}
}

@article{oleary2015thermal-efficiency,
 abstract = {Abstract This paper investigates the use of actual monitored household energy as an indicator of the thermal efficiency of a dwelling and subsequently rating of the building thermal performance. The paper reviews evaluation methods used internationally for both building thermal efficiency and building energy labelling and presents results from two discrete studies in South Australia on monitoring actual household energy consumption. In order to investigate the occupancy effect on household energy, monitored energy data collected from two different housing developments in South Australia were examined. The energy ratings for these homes are compliant with the national agreed protocols for thermal performance modelling of dwellings, where one set of homes is a group occupied by higher socio-economic groups and the other is low income public housing in a colder climate region with much poorer home energy ratings. The wide variation of actual household energy for the homes that have relatively similar thermal envelopes indicates a lack of meaningful use for actual household energy in disclosure of house energy performance. Therefore, it is argued that thermal modelling software used to rate homes appears a more useful application of a system of disclosure of energy performance than the use of energy bills.},
 author = {Timothy O’Leary and M. Belusko and D. Whaley and F. Bruno},
 doi = {10.1016/j.enbuild.2015.09.018},
 file = {oleary2015thermal-efficiency.pdf:oleary2015thermal-efficiency.pdf:PDF},
 issn = {0378-7788},
 journal = {Energy and Buildings},
 keywords = {Monitored energy use, energy, thermal, efficiency, comfort, behaviour},
 pages = {433 - 440},
 title = {Review and evaluation of using household metered energy data for rating of building thermal efficiency of existing buildings},
 volume = {108},
 year = {2015}
}

@inproceedings{parson2015dataport,
 __markedentry = {[Jack:6]},
 author = {Parson, Oliver and Fisher, Grant and Hersey, April and Batra, Nipun and Kelly, Jack and Singh, Amarjeet and Knottenbelt, William and Rogers, Alex},
 booktitle = {1st International Symposium on Signal Processing Applications in Smart Buildings at 3rd IEEE Global Conference on Signal \& Information Processing (GlobalSIP'15)},
 eventdate = {14-16 December 2015},
 keywords = {energy, NILM, NILMTK},
 location = {Orlando, FL, USA},
 month = {December},
 organization = {IEEE},
 owner = {Jack},
 timestamp = {2015.11.19},
 title = {Dataport and {NILMTK}: A building data set designed for non-intrusive load monitoring},
 year = {2015}
}

@inproceedings{pereira2015openEDF,
 abstract = {In this paper we present our approach to create an end-to-end software platform to enable the creation of meaningful and systematic, cross-dataset performance evaluations and benchmarks of Non-Intrusive Load Monitoring technology. We specifically propose a new file format to represent public datasets, a software framework to implement algorithms and metrics as well as the application of ceiling analysis to evaluate the overall performance of NILM systems.},
 author = {Pereira, Lucas and Nunes, Nuno J},
 booktitle = {Sustainable Internet and ICT for Sustainability (SustainIT), 2015},
 file = {pereira2015openEDF.pdf:pereira2015openEDF.pdf:PDF},
 keywords = {NILM, open-source, file format, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {1--3},
 timestamp = {2015.05.13},
 title = {Towards Systematic Performance Evaluation of Non-Intrusive Load Monitoring Algorithms and Systems},
 year = {2015}
}

@article{pybrain2010jmlr,
 author = {Schaul, Tom and Bayer, Justin and Wierstra, Daan and Sun, Yi and Felder, Martin and Sehnke, Frank and R{\"u}ckstie{\ss}, Thomas and Schmidhuber, J{\"u}rgen},
 journal = {Journal of Machine Learning Research},
 keywords = {machine learning, software, open-source, LSTM},
 pages = {743--746},
 title = {PyBrain},
 volume = {11},
 year = {2010}
}

@article{rezende2014stochastic,
 author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
 file = {rezende2014stochastic.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/rezende2014stochastic.pdf:PDF},
 keywords = {DeepMind, generative},
 owner = {jack},
 timestamp = {2015.03.30},
 title = {Stochastic backpropagation and approximate inference in deep generative models},
 year = {2014}
}

@inproceedings{rifai2011contractive,
 abstract = {We present in this paper a novel approach
for training deterministic auto-encoders. We
show that by adding a well chosen penalty
term to the classical reconstruction cost function, we can achieve results that equal or surpass those
attained by other regularized auto-encoders
as well as denoising auto-encoders
on a range of datasets. This penalty term
corresponds to the Frobenius norm of the
Jacobian matrix of the encoder activations
with respect to the input. We show that
this penalty term results in a localized space
contraction which in turn yields robust features
on the activation layer. Furthermore, we show how this penalty term is related to
both regularized auto-encoders and denoising
auto-encoders and how it can be seen as a link
between deterministic and non-deterministic
auto-encoders. We nd empirically that this
penalty helps to carve a representation that
better captures the local directions of variation
dictated by the data, corresponding to a
lower-dimensional non-linear manifold, while
being more invariant to the vast majority of
directions orthogonal to the manifold.
Finally, we show that by using the learned
features to initialize a MLP, we achieve state
of the art classication error on a range of
datasets, surpassing other methods of pre-training.},
 author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
 booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
 file = {rifai2011contractive-auto-encoders.pdf:rifai2011contractive-auto-encoders.pdf:PDF},
 keywords = {autoencoder},
 owner = {Jack},
 pages = {833--840},
 timestamp = {2015.05.01},
 title = {Contractive auto-encoders: Explicit invariance during feature extraction},
 year = {2011}
}

@article{rolfe2013discriminative-recurrent-sparse-auto-encoders,
 abstract = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters.
From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
 author = {Rolfe, Jason Tyler and LeCun, Yann},
 eprint = {1301.3775},
 eprinttype = {arXiv},
 file = {rolfe2013discriminative-recurrent-sparse-auto-encoders.pdf:rolfe2013discriminative-recurrent-sparse-auto-encoders.pdf:PDF},
 keywords = {autoencoder, recurrent, discriminative},
 owner = {Jack},
 timestamp = {2015.04.21},
 title = {Discriminative Recurrent Sparse Auto-Encoders},
 year = {2013}
}

@inproceedings{roos1994neural-nets-nilm,
 abstract = {The success of demand side energy control in industries, mines and commercial buildings depends on factors like the energy sensitivity and awareness of the organisation as well as an accurate and effective measurement and monitoring of its electrical energy consumption. Demand side energy control also forms an important part in the research programs of many research organisations. Reliable data on energy consumption is therefore imperative for effective research in this field, as well as for the successful implementation of demand side management. Traditional load research instrumentation has involved intrusive techniques that require the installation of sensors on each of the individual components of the total load. A non-intrusive appliance load monitor is proposed in this paper to determine the energy consumption of individual appliances turning on or off or operating under continuously varying load conditions. This monitoring system, which is implemented by network pattern identification technology, is based on detailed analysis of the current and voltage of the total load, as measured at the interface of the power source. The approach has been developed to simplify the collection of energy consumption data by utilities, but also has other applications. It is called nonintrusive to contrast it with previous techniques for gathering appliance data, which require placing sensors on individual appliances, and hence an intrusion onto the energy consumer's property.},
 author = {Roos, JG and Lane, IE and Botha, EC and Hancke, Gerhard P},
 booktitle = {Instrumentation and Measurement Technology Conference, 1994. IMTC/94. Conference Proceedings. 10th Anniversary. Advanced Technologies in I \& M., 1994 IEEE},
 comment = {just a proposal!!!},
 doi = {10.1109/IMTC.1994.351862},
 file = {roos1994neural-nets-nilm.pdf:roos1994neural-nets-nilm.pdf:PDF},
 keywords = {NILM, neural networks, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {1115--1118},
 timestamp = {2015.07.22},
 title = {Using neural networks for non-intrusive monitoring of industrial electrical loads},
 year = {1994}
}

@techreport{rumelhart1985learning,
 author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
 institution = {DTIC Document},
 keywords = {neural networks},
 owner = {Jack},
 timestamp = {2015.07.23},
 title = {Learning internal representations by error propagation},
 year = {1985}
}

@misc{russakovsky14imagenet,
 author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
 eprint = {1409.0575},
 eprinttype = {arXiv},
 file = {russakovsky14imagenet.pdf:russakovsky14imagenet.pdf:PDF},
 keywords = {data, machine learning, image classification},
 owner = {Jack},
 timestamp = {2014.11.12},
 title = {ImageNet Large Scale Visual Recognition Challenge},
 year = {2014}
}

@inproceedings{ruzzelli2010,
 abstract = {Sensing, monitoring and actuating systems are expected to play a key role in reducing buildings overall energy consumption. Leveraging sensor systems to support energy efficiency in buildings poses novel research challenges in monitoring space usage, controlling devices, interfacing with smart energy meters and communicating with the energy grid. In the attempt of reducing electricity consumption in buildings, identifying individual sources of energy consumption is key to generate energy awareness and improve efficiency of available energy resources usage. Previous work studied several non-intrusive load monitoring techniques to classify appliances; however, the literature lacks of an comprehensive system that can be easily installed in existing buildings to empower users profiling, benchmarking and recognizing loads in real-time. This has been a major reason holding back the practice adoption of load monitoring techniques. In this paper we present RECAP: RECognition of electrical Appliances and Profiling in real-time. RECAP uses a single wireless energy monitoring sensor easily clipped to the main electrical unit. The energy monitoring unit transmits energy data wirelessly to a local machine for data processing and storage. The RECAP system consists of three parts: (1) Guiding the user for profiling electrical appliances within premises and generating a database of unique appliance signatures; (2) Using those signatures to train an artificial neural network that is then employed to recognize appliance activities (3) Providing a Load descriptor to allow peer appliance benchmarking. RECAP addresses the need of an integrated and intuitive tool to empower building owners with energy awareness. Enabling real-time appliance recognition is a stepping-stone towards reducing energy consumption and allowing a number of major applications including load-shifting techniques, energy expenditure breakdown per appliance, detection of power hungry and faulty appliances, and recogn- - ition of occupant activity. This paper describes the system design and performance evaluation in domestic environment.},
 author = {Ruzzelli, Antonio G and Nicolas, C and Schoofs, Anthony and O'Hare, Gregory MP},
 booktitle = {Sensor Mesh and Ad Hoc Communications and Networks (SECON), 2010 7th Annual IEEE Communications Society Conference on},
 doi = {10.1109/SECON.2010.5508244},
 file = {ruzzelli2010.pdf:ruzzelli2010.pdf:PDF},
 keywords = {NILM, neural networks, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {1--9},
 timestamp = {2015.07.22},
 title = {Real-time recognition and profiling of appliances through a single electricity sensor},
 year = {2010}
}

@article{schmidhuber2014deep-learning-overview,
 abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
 author = {J{\"u}rgen Schmidhuber},
 comments = {88 pages, 888 references},
 doi = {10.1016/j.neunet.2014.09.003},
 eprint = {1404.7828},
 file = {:schmidhuber2014deep-learning-overview.pdf:PDF},
 journal = {Neural Networks},
 keywords = {deep learning, review},
 month = {apr},
 oai2identifier = {1404.7828},
 owner = {jack},
 reportno = {Technical Report IDSIA-03-14},
 timestamp = {2014.10.23},
 title = {Deep Learning in Neural Networks: An Overview},
 url = {http://people.idsia.ch/~juergen/deep-learning-overview.html},
 year = {2014}
}

@article{sonderby2015RNN-SPN,
 abstract = {We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest.},
 author = {S{\o}nderby, S{\o}ren Kaae and S{\o}nderby, Casper Kaae and Maal{\o}e, Lars and Winther, Ole},
 comment = {Related: jaderberg2015STN},
 eprint = {1509.05329},
 eprinttype = {arXiv},
 file = {sonderby2015RNN-SPN.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/sonderby2015RNN-SPN.pdf:PDF},
 keywords = {deep learning, spatial transformer networks},
 owner = {jack},
 timestamp = {2015.09.23},
 title = {Recurrent Spatial Transformer Networks},
 year = {2015}
}

@article{sorrell2015reducing,
 abstract = {Abstract Most commentators expect improved energy efficiency and reduced energy demand to provide the dominant contribution to tackling global climate change. But at the global level, the correlation between increased wealth and increased energy consumption is very strong and the impact of policies to reduce energy demand is both limited and contested. Different academic disciplines approach energy demand reduction in different ways: emphasising some mechanisms and neglecting others, being more or less optimistic about the potential for reducing energy demand and providing insights that are more or less useful for policymakers. This article provides an overview of the main issues and challenges associated with energy demand reduction, summarises how this challenge is ‘framed’ by key academic disciplines, indicates how these can provide complementary insights for policymakers and argues that a ‘sociotechnical’ perspective can provide a deeper understanding of the nature of this challenge and the processes through which it can be achieved. The article integrates ideas from the natural sciences, economics, psychology, innovation studies and sociology but does not give equal weight to each. It argues that reducing energy demand will prove more difficult than is commonly assumed and current approaches will be insufficient to deliver the transformation required.},
 author = {Steve Sorrell},
 doi = {10.1016/j.rser.2015.03.002},
 file = {sorrell2015reducing.pdf:sorrell2015reducing.pdf:PDF},
 issn = {1364-0321},
 journaltitle = {Renewable and Sustainable Energy Reviews},
 keywords = {energy, psychology, energy demand reduction},
 owner = {Jack},
 pages = {74 - 82},
 timestamp = {2015.06.24},
 title = {Reducing energy demand: A review of issues, challenges and approaches},
 volume = {47},
 year = {2015}
}

@inproceedings{staudemeyer2013evaluating-LSTM,
 acmid = {2513490},
 address = {New York, NY, USA},
 author = {Staudemeyer, Ralf C. and Omlin, Christian W.},
 booktitle = {Proceedings of the South African Institute for Computer Scientists and Information Technologists Conference},
 doi = {10.1145/2513456.2513490},
 isbn = {978-1-4503-2112-9},
 keywords = {KDDCup99, intrusion detection systems, long short-term memory, machine learning, receiver operating characteristic, recurrent neural networks, RNNs, LSTM},
 location = {East London, South Africa},
 numpages = {7},
 owner = {Jack},
 pages = {218--224},
 publisher = {ACM},
 series = {SAICSIT '13},
 timestamp = {2014.10.30},
 title = {Evaluating Performance of Long Short-term Memory Recurrent Neural Networks on Intrusion Detection Data},
 year = {2013}
}

@inproceedings{sutskever2014machine-translation,
 archiveprefix = {arXiv},
 author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
 booktitle = {Advances in Neural Information Processing Systems 27},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
 eprint = {1409.3215},
 eprinttype = {arXiv},
 file = {:sutskever2014machine-translation.pdf:PDF},
 keywords = {LSTM, language, machine translation},
 notes = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks},
 pages = {3104--3112},
 publisher = {Curran Associates, Inc.},
 pubstate = {forthcoming},
 title = {Sequence to Sequence Learning with Neural Networks},
 year = {2014}
}

@article{szegedy2014going-deeper,
 author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
 eprint = {1409.4842},
 eprinttype = {arXiv},
 file = {szegedy2014going-deeper.pdf:szegedy2014going-deeper.pdf:PDF},
 keywords = {deep learning},
 owner = {Jack},
 timestamp = {2014.11.12},
 title = {Going deeper with convolutions},
 year = {2014}
}

@inproceedings{tokuda2000speech,
 author = {Tokuda, Keiichi and Yoshimura, Takayoshi and Masuko, Takashi and Kobayashi, Takao and Kitamura, Tadashi},
 booktitle = {Acoustics, Speech, and Signal Processing, 2000. ICASSP'00. Proceedings. 2000 IEEE International Conference on},
 doi = {10.1109/ICASSP.2000.861820},
 file = {tokuda2000speech.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/tokuda2000speech.pdf:PDF},
 keywords = {speech, HMMs},
 organization = {IEEE},
 owner = {jack},
 pages = {1315--1318},
 timestamp = {2015.03.20},
 title = {Speech parameter generation algorithms for HMM-based speech synthesis},
 volume = {3},
 year = {2000}
}

@article{tsodyks2002STDP,
 author = {Tsodyks, Misha},
 file = {tsodyks2002STDP.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/tsodyks2002STDP.pdf:PDF},
 journal = {TRENDS in Neurosciences},
 keywords = {neuroscience, STDP, synaptic plasticity},
 number = {12},
 owner = {jack},
 pages = {599--600},
 publisher = {Elsevier},
 timestamp = {2015.03.21},
 title = {Spike-timing-dependent synaptic plasticity--The long road towards understanding neuronal mechanisms of learning and memory},
 volume = {25},
 year = {2002}
}

@article{UK-DALE,
 author = {Jack Kelly and William Knottenbelt},
 date = {2015/03/31},
 doi = {10.1038/sdata.2015.7},
 eprint = {1404.0284},
 eprinttype = {arXiv},
 file = {UK-DALE.pdf:UK-DALE.pdf:PDF},
 journal = {Scientific Data},
 journaltitle = {Scientific Data},
 keywords = {data, NILM, energy},
 number = {150007},
 owner = {Jack},
 timestamp = {2015.03.09},
 title = {The {UK-DALE} dataset, domestic appliance-level electricity demand and whole-house demand from five UK homes},
 volume = {2},
 year = {2015}
}

@inproceedings{uria2012articulatory-inversion,
 author = {Benigno Uria and Iain Murray and Steve Renals and Korin Richmond},
 booktitle = {Proceedings of Interspeech},
 file = {uria2012articulatory-inversion.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/uria2012articulatory-inversion.pdf:PDF},
 keywords = {density estimation},
 owner = {jack},
 timestamp = {2015.03.20},
 title = {Deep architectures for articulatory inversion},
 year = {2012}
}

@article{uria2014density-estimator,
 author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
 file = {uria2014density-estimator.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/uria2014density-estimator.pdf:PDF},
 journal = {JMLR: W\&CP},
 keywords = {density estimation, deep learning},
 number = {1},
 owner = {jack},
 pages = {467--475},
 timestamp = {2015.03.20},
 title = {A Deep and Tractable Density Estimator},
 volume = {32},
 year = {2014}
}

@article{uria2015trajectory-RNADE,
 author = {Uria, Benigno and Murray, Iain and Renals, Steve and Valentini, Cassia and Bridle, John},
 file = {uria2015trajectory-RNADE.pdf:home/jack/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/uria2015trajectory-RNADE.pdf:PDF},
 journal = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2015},
 keywords = {density estimation},
 owner = {jack},
 timestamp = {2015.03.20},
 title = {Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE},
 year = {2015}
}

@inproceedings{vincent2008denoising-autoencoders,
 author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
 booktitle = {Proceedings of the 25th international conference on Machine learning},
 file = {vincent2008denoising-autoencoders.pdf:vincent2008denoising-autoencoders.pdf:PDF},
 keywords = {neural networks, autoencoder},
 organization = {ACM},
 owner = {Jack},
 pages = {1096--1103},
 timestamp = {2015.07.23},
 title = {Extracting and composing robust features with denoising autoencoders},
 year = {2008}
}

@article{wahlstrom2014deep-dynamical,
 author = {Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B and Deisenroth, Marc Peter},
 eprint = {1410.7550},
 eprinttype = {arXiv},
 file = {wahlstrom2014deep-dynamical.pdf:wahlstrom2014deep-dynamical.pdf:PDF},
 keywords = {machine learning, deep neural networks},
 owner = {Jack},
 timestamp = {2015.04.09},
 title = {Learning deep dynamical models from image pixels},
 year = {2014}
}

@inproceedings{weninger2014RNN-de-reverb,
 abstract = {This paper describes our joint efforts to provide robust automatic
speech recognition (ASR) for reverberated environments, such as in
hands-free human-machine interaction. We investigate blind feature
space de-reverberation and deep recurrent de-noising auto-encoders
(DAE) in an early fusion scheme. Results on the 2014 REVERB
Challenge development set indicate that the DAE front-end provides
complementary performance gains to multi-condition training, feature
transformations, and model adaptation. The proposed ASR system
achieves word error rates of 17.62% and 36.6% on simulated and real
data, which is a significant improvement over the Challenge baseline
(25.16 and 47.2%)},
 author = {Weninger, Felix and Watanabe, Shinji and Tachioka, Yuuki and Schuller, Bjorn},
 booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
 file = {weninger2014RNN-de-reverb.pdf:weninger2014RNN-de-reverb.pdf:PDF},
 keywords = {RNN, recurrent, speech, ASR, autoencoder, DRDAE},
 organization = {IEEE},
 owner = {Jack},
 pages = {4623--4627},
 timestamp = {2015.04.21},
 title = {Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition},
 year = {2014}
}

@article{werbos1988generalization,
 author = {Werbos, Paul J},
 journal = {Neural Networks},
 journaltitle = {Neural Networks},
 keywords = {neural networks},
 number = {4},
 owner = {Jack},
 pages = {339--356},
 publisher = {Elsevier},
 timestamp = {2015.07.23},
 title = {Generalization of backpropagation with application to a recurrent gas market model},
 volume = {1},
 year = {1988}
}

@article{werbos1990BPTT,
 author = {Werbos, Paul J},
 journal = {Proceedings of the {IEEE}},
 journaltitle = {Proceedings of the IEEE},
 keywords = {neural networks, recurrent neural networks},
 number = {10},
 owner = {Jack},
 pages = {1550--1560},
 publisher = {IEEE},
 timestamp = {2015.07.23},
 title = {Backpropagation through time: what it does and how to do it},
 volume = {78},
 year = {1990}
}

@article{williams1995gradient,
 author = {Williams, Ronald J and Zipser, David},
 journal = {Back-propagation: Theory, architectures and applications},
 journaltitle = {Back-propagation: Theory, architectures and applications},
 keywords = {neural networks},
 owner = {Jack},
 pages = {433--486},
 timestamp = {2015.07.23},
 title = {Gradient-based learning algorithms for recurrent networks and their computational complexity},
 year = {1995}
}

@inproceedings{yang2007neural-net-nilm,
 abstract = {This paper proposes to compare the performance of neural network classifiers between back propagation (BP) and learning vector quantization (LVQ) for pattern analyses of features selection in a non-intrusive load monitoring (NILM) system. Load recognition for identifying loads being connected and disconnected is applied to a NILM by using a neural network, especially for industrial electrical loads, even though some loads are activated at the nearly same time. In order to accurately decompose the aggregate load into its components, a feature-based model for describing the signatures of individual appliances and load combinations is used. The model will suggest the certain signatures which can be detected for all loads in order to indicate the activities of the separate components. To verify the performance of the model for the features selection, the data sets of the electrical loads and the load recognition techniques apply an electromagnetic transient program (EMTP) and a neural network, respectively. The effectiveness and computation equipment of load recognition are analyzed and compared by using the back propagation classifier and the learning vector quantization classifier. To obtain a maximum recognition accuracy rate, the calculation of the turn-on transient energy signature employs a window of samples, At, to adaptively segment a transient representative of a class of loads. Experiments performed with a variety of model data sets which reveal the back propagation classifier is superior to the learning quantization classifier in the effectiveness and computation equipment of load recognition.},
 author = {Yang, Hong-Tzer and Chang, Hsueh-Hsien and Lin, Ching-Lung},
 booktitle = {Computer Supported Cooperative Work in Design, 2007. CSCWD 2007. 11th International Conference on},
 doi = {10.1109/CSCWD.2007.4281579},
 file = {yang2007neural-net-nilm.pdf:yang2007neural-net-nilm.pdf:PDF},
 keywords = {NILM, neural networks, energy},
 organization = {IEEE},
 owner = {Jack},
 pages = {1022--1027},
 timestamp = {2015.07.22},
 title = {Design a neural network for features selection in non-intrusive monitoring of industrial electrical loads},
 year = {2007}
}

@article{zaremba2014execute,
 archiveprefix = {arXiv},
 author = {Wojciech Zaremba and Ilya Sutskever},
 eprint = {1410.4615},
 eprinttype = {arXiv},
 file = {:zaremba2014execute.pdf:PDF},
 keywords = {LSTM, Python},
 owner = {jack},
 timestamp = {2014.10.22},
 title = {Learning to Execute},
 year = {2014}
}

@incollection{zeiler2014visualizing,
 abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
 author = {Zeiler, Matthew D and Fergus, Rob},
 booktitle = {Computer Vision--ECCV 2014},
 eprint = {1311.2901},
 eprinttype = {arXiv},
 file = {zeiler2014visualizing.pdf:/home/dk3810/Dropbox/MyWork/imperial/PhD/writing/jabref_bibliography/zeiler2014visualizing.pdf:PDF},
 keywords = {convolutional neural networks, deconvolutional, visualisation},
 owner = {Jack},
 pages = {818--833},
 publisher = {Springer},
 timestamp = {2015.05.01},
 title = {Visualizing and understanding convolutional networks},
 year = {2014}
}

